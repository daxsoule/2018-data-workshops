{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Sergey_Molodtsov_ADCP_Quality_Report_Global_Irminger_Sea.ipynb",
   "version": "0.3.2",
   "views": {},
   "default_view": {},
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "OOI",
   "language": "python",
   "name": "ooi"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "two## Data Quality Report ADCP, Global Irminger Sea\n",
    "\n",
    "Evaluation Date: 5/24/2018\n",
    "\n",
    "Evaluator: Sergey Molodtsov\n",
    "\n",
    "Review Summary\n",
    "\n",
    "This report assesses the review the Global Irminger Sea Mooring ADCPs. I focus on data available from the moorings with installled ADCP, and compare the ADCP results between two different moorings during 2014 - 2018 time period (if data is available). \n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Selected instruments included in this report\n",
    "In this report, we will evaluate the ADCP from the Global Irminger Sea focusing on the full datasets available from Flanking Mooring A (GI03FLMA) and Mooring B (GI03FLMB). \n",
    "\n",
    "Instrument | Reference Designator | Method | Stream \n",
    " -- | -- | -- | --\n",
    "75 kHz ADCP | [GI03FLMA-RIM01-02-ADCPSL003](http://ooi.visualocean.net/instruments/view/GI03FLMA-RIM01-02-ADCPSL003) | recovered_inst | adcp_velocity_earth\n",
    "75 kHz ADCP | [GI03FLMB-RIM01-02-ADCPSL007](http://ooi.visualocean.net/instruments/view/GI03FLMB-RIM01-02-ADCPSL007) | recovered_inst | adcp_velocity_earth\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Time periods of interest\n",
    "We will focus on the following time periods for evaluation:\n",
    "* Sep 2014 to Aug 2017 - All of Deployment Flanking Mooring A\n",
    "* Sep 2014 to Aug 2017 - All of Deployment Flanking Mooring B\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Related Metadata\n",
    "In this section, we will review some of metadata available in the system to make sure it is present and correct.\n",
    "\n",
    "Before we get started, we need to set up our Python environment with some libraries, variables and functions we will need later in this report."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# API Information\n",
    "USERNAME ='OOIAPI-D8S960UXPK4K03'\n",
    "TOKEN= 'IXL48EQ2XY'\n",
    "DATA_API = 'https://ooinet.oceanobservatories.org/api/m2m/12576/sensor/inv'\n",
    "VOCAB_API = 'https://ooinet.oceanobservatories.org/api/m2m/12586/vocab/inv'\n",
    "ASSET_API = 'https://ooinet.oceanobservatories.org/api/m2m/12587'\n",
    "\n",
    "#GI03FLMA-RIM01-02-ADCPSL003\n",
    "site = 'GI03FLMA'\n",
    "node = 'RIM01'\n",
    "instrument = '02-ADCPSL003'\n",
    "method = 'recovered_inst'\n",
    "stream = 'adcp_velocity_earth'\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Specify some functions to convert timestamps\n",
    "ntp_epoch = datetime.datetime(1900, 1, 1)\n",
    "unix_epoch = datetime.datetime(1970, 1, 1)\n",
    "ntp_delta = (unix_epoch - ntp_epoch).total_seconds()\n",
    "\n",
    "def ntp_seconds_to_datetime(ntp_seconds):\n",
    "    return datetime.datetime.utcfromtimestamp(ntp_seconds - ntp_delta).replace(microsecond=0)\n",
    "  \n",
    "def convert_time(ms):\n",
    "  if ms != None:\n",
    "    return datetime.datetime.utcfromtimestamp(ms/1000)\n",
    "  else:\n",
    "    return None\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3a. ADCP Flanking Mooring A\n",
    "\n",
    "Vocabulary Metadata\n",
    "First, let's grab the basic vocabulary information (metadata) from the system to make sure we have the right instrument.\n",
    "\n",
    " "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Setup the API request url\n",
    "data_request_url ='/'.join((VOCAB_API,site,node,instrument))\n",
    "print data_request_url\n",
    "\n",
    "# Grab the information from the server\n",
    "r = requests.get(data_request_url, auth=(USERNAME, TOKEN))\n",
    "data = r.json()\n",
    "data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Deployment Information\n",
    "Next, let's grab some information about the deployments for this instrument.  We will grab all of the deployments available in the system since 2014 and then output the date ranges, latitude/longitude, asset ID, and sensor ID for each.  Note that the **reference designator** specified above represents the geographical location of an instrument across all deployments (e.g. the ADCP on the Irminger Moorings), the **Sensor ID** (and its Asset ID equivalent) represent the specific instrument used for a given deployment (i.e. a unique make, model, and serial numbered instrument)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Setup the API request url\n",
    "data_request_url = ASSET_API + '/events/deployment/query'\n",
    "params = {\n",
    "  'beginDT':'2014-01-01T00:00:00.000Z',\n",
    "  'endDT':'2018-01-01T00:00:00.000Z',\n",
    "  'refdes':site+'-'+node+'-'+instrument,   \n",
    "}\n",
    "\n",
    "# Grab the information from the server\n",
    "r = requests.get(data_request_url, params=params, auth=(USERNAME, TOKEN))\n",
    "data = r.json()\n",
    "\n",
    "df = pd.DataFrame() # Setup empty array\n",
    "for d in data:\n",
    "  df = df.append({\n",
    "      'deployment': d['deploymentNumber'],\n",
    "      'start': convert_time(d['eventStartTime']),\n",
    "      'stop': convert_time(d['eventStopTime']),\n",
    "      'latitude': d['location']['latitude'],\n",
    "      'longitude': d['location']['longitude'],\n",
    "      'sensor': d['sensor']['uid'],\n",
    "      'asset_id': d['sensor']['assetId'],\n",
    "    }, ignore_index=True)\n",
    "df"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Annotations\n",
    "Finally, let's pull any relevant annotations for the ADCP instrument at Mooring A."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ANNO_API = 'https://ooinet.oceanobservatories.org/api/m2m/12580/anno/find'\n",
    "params = {\n",
    "  'beginDT':int(datetime.date(2017,1,1).strftime('%s'))*1000,\n",
    "  'endDT':int(datetime.date(2018,1,1).strftime('%s'))*1000,\n",
    "  'refdes':site+'-'+node+'-'+instrument,\n",
    "}\n",
    "\n",
    "r = requests.get(ANNO_API, params=params, auth=(USERNAME, TOKEN))\n",
    "data = r.json()\n",
    "\n",
    "df = pd.DataFrame() # Setup empty array\n",
    "for d in data:\n",
    "  df = df.append({\n",
    "    'annotation': d['annotation'],\n",
    "    'start': convert_time(d['beginDT']),\n",
    "    'stop': convert_time(d['endDT']),\n",
    "    'site': d['subsite'],\n",
    "    'node': d['node'],\n",
    "    'sensor': d['sensor'],\n",
    "    'id': d['id']\n",
    "  }, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', -1) # Show the full annotation text\n",
    "df"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3b. ADCP Flanking Mooring B\n",
    "\n",
    "Vocabulary Metadata\n",
    "First, let's grab the basic vocabulary information (metadata) from the system to make sure we have the right instrument.\n",
    "\n",
    " "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#GI03FLMA-RIM01-02-ADCPSL003\n",
    "\n",
    "method = 'recovered_inst'\n",
    "stream = 'adcp_velocity_earth'\n",
    "site = 'GI03FLMB'\n",
    "node = 'RIM01'\n",
    "instrument = '02-ADCPSL007'\n",
    "\n",
    "data_request_url ='/'.join((VOCAB_API,site,node,instrument))\n",
    "print data_request_url\n",
    "\n",
    "# Grab the information from the server\n",
    "r = requests.get(data_request_url, auth=(USERNAME, TOKEN))\n",
    "data = r.json()\n",
    "data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Deployment Information\n",
    "Next, let's grab some information about the deployments for this instrument.  We will grab all of the deployments available in the system since 2014 and then output the date ranges, latitude/longitude, asset ID, and sensor ID for each.  Note that the **reference designator** specified above represents the geographical location of an instrument across all deployments (e.g. the ADCP on the Irminger Moorings), the **Sensor ID** (and its Asset ID equivalent) represent the specific instrument used for a given deployment (i.e. a unique make, model, and serial numbered instrument)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Setup the API request url\n",
    "data_request_url = ASSET_API + '/events/deployment/query'\n",
    "params = {\n",
    "  'beginDT':'2014-01-01T00:00:00.000Z',\n",
    "  'endDT':'2018-01-01T00:00:00.000Z',\n",
    "  'refdes':site+'-'+node+'-'+instrument,   \n",
    "}\n",
    "\n",
    "# Grab the information from the server\n",
    "r = requests.get(data_request_url, params=params, auth=(USERNAME, TOKEN))\n",
    "data = r.json()\n",
    "\n",
    "df = pd.DataFrame() # Setup empty array\n",
    "for d in data:\n",
    "  df = df.append({\n",
    "      'deployment': d['deploymentNumber'],\n",
    "      'start': convert_time(d['eventStartTime']),\n",
    "      'stop': convert_time(d['eventStopTime']),\n",
    "      'latitude': d['location']['latitude'],\n",
    "      'longitude': d['location']['longitude'],\n",
    "      'sensor': d['sensor']['uid'],\n",
    "      'asset_id': d['sensor']['assetId'],\n",
    "    }, ignore_index=True)\n",
    "df"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Annotations\n",
    "Finally, let's pull any relevant annotations for the ADCP instrument at Mooring B."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ANNO_API = 'https://ooinet.oceanobservatories.org/api/m2m/12580/anno/find'\n",
    "params = {\n",
    "  'beginDT':int(datetime.date(2014,1,1).strftime('%s'))*1000,\n",
    "  'endDT':int(datetime.date(2018,1,1).strftime('%s'))*1000,\n",
    "  'refdes':site+'-'+node+'-'+instrument,\n",
    "}\n",
    "\n",
    "r = requests.get(ANNO_API, params=params, auth=(USERNAME, TOKEN))\n",
    "data = r.json()\n",
    "\n",
    "df = pd.DataFrame() # Setup empty array\n",
    "for d in data:\n",
    "  df = df.append({\n",
    "    'annotation': d['annotation'],\n",
    "    'start': convert_time(d['beginDT']),\n",
    "    'stop': convert_time(d['endDT']),\n",
    "    'site': d['subsite'],\n",
    "    'node': d['node'],\n",
    "    'sensor': d['sensor'],\n",
    "    'id': d['id']\n",
    "  }, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', -1) # Show the full annotation text\n",
    "df"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. The full dataset\n",
    "Now let's take a look at a full dataset for each of the Moorings.\n",
    "\n",
    "Login in at https://ooinet.oceanobservatories.org/ and obtain your <b>API username and API token</b> under your profile (top right corner), or use the credentials provided below.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "username = 'OOIAPI-D8S960UXPK4K03'\n",
    "token = 'IXL48EQ2XY'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Specify your inputs (flanking Mooring A)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#GI03FLMA-RIM01-02-ADCPSL003\n",
    "subsite = 'GI03FLMA'\n",
    "node = 'RIM01'\n",
    "sensor = '02-ADCPSL003'\n",
    "method = 'recovered_inst'\n",
    "stream = 'adcp_velocity_earth'\n",
    "beginDT = '2014-09-27T10:32:51.600Z'\n",
    "endDT = '2018-05-20T10:32:51.600Z'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Request the data from flanking Mooring A."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "base_url = 'https://ooinet.oceanobservatories.org/api/m2m/12576/sensor/inv/'\n",
    "\n",
    "data_request_url ='/'.join((base_url,subsite,node,sensor,method,stream))\n",
    "params = {\n",
    "    'beginDT':beginDT,\n",
    "    'endDT':endDT,   \n",
    "}\n",
    "r = requests.get(data_request_url, params=params, auth=(username, token))\n",
    "data_A = r.json()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Specify your inputs (flanking Mooring B)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#GI03FLMB-RIM01-02-ADCPSL007\n",
    "\n",
    "subsite = 'GI03FLMB'\n",
    "node = 'RIM01'\n",
    "sensor = '02-ADCPSL007'\n",
    "method = 'recovered_inst'\n",
    "stream = 'adcp_velocity_earth'\n",
    "beginDT = '2014-09-27T10:32:51.600Z'\n",
    "endDT = '2018-05-20T10:32:51.600Z'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Request the data from flanking Mooring B."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "base_url = 'https://ooinet.oceanobservatories.org/api/m2m/12576/sensor/inv/'\n",
    "\n",
    "data_request_url ='/'.join((base_url,subsite,node,sensor,method,stream))\n",
    "params = {\n",
    "    'beginDT':beginDT,\n",
    "    'endDT':endDT,   \n",
    "}\n",
    "r = requests.get(data_request_url, params=params, auth=(username, token))\n",
    "data_B = r.json()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#print data\n",
    "print(data_A['allURLs'][0])\n",
    "print(data_B['allURLs'][0])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Check if the request has completed."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%time\n",
    "check_complete = data_A['allURLs'][1] + '/status.txt'\n",
    "for i in range(1800): \n",
    "    r = requests.get(check_complete)\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        print('request completed')\n",
    "        break\n",
    "    else:\n",
    "        time.sleep(1)\n",
    "check_complete = data_B['allURLs'][1] + '/status.txt'\n",
    "for i in range(1800): \n",
    "    r = requests.get(check_complete)\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        print('request completed')\n",
    "        break\n",
    "    else:\n",
    "        time.sleep(1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "!pip install xarray\n",
    "!pip install netcdf4\n",
    "import re\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Parse the thredds server to get a list of all NetCDF files mooring A"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "url = data_A['allURLs'][0]\n",
    "tds_url = 'https://opendap.oceanobservatories.org/thredds/dodsC'\n",
    "datasets = requests.get(url).text\n",
    "urls = re.findall(r'href=[\\'\"]?([^\\'\" >]+)', datasets)\n",
    "x = re.findall(r'(ooi/.*?.nc)', datasets)\n",
    "for i in x:\n",
    "    if i.endswith('.nc') == False:\n",
    "        x.remove(i)\n",
    "for i in x:\n",
    "    try:\n",
    "        float(i[-4])\n",
    "    except:\n",
    "        x.remove(i)\n",
    "datasets_A = [os.path.join(tds_url, i) for i in x]\n",
    "datasets_A"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Parse the thredds server to get a list of all NetCDF files mooring B"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "url = data_B['allURLs'][0]\n",
    "tds_url = 'https://opendap.oceanobservatories.org/thredds/dodsC'\n",
    "datasets = requests.get(url).text\n",
    "urls = re.findall(r'href=[\\'\"]?([^\\'\" >]+)', datasets)\n",
    "x = re.findall(r'(ooi/.*?.nc)', datasets)\n",
    "for i in x:\n",
    "    if i.endswith('.nc') == False:\n",
    "        x.remove(i)\n",
    "for i in x:\n",
    "    try:\n",
    "        float(i[-4])\n",
    "    except:\n",
    "        x.remove(i)\n",
    "datasets_B = [os.path.join(tds_url, i) for i in x]\n",
    "datasets_B"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install dask\n",
    "import dask"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Load the list of NetCDF files into xarray. Note that this datasets has two dimensions, time and bin. Some variables are dimensioned along just time, others along both time and bin number."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#ds = xr.open_dataset(data_test)\n",
    "ds_A = xr.open_mfdataset(datasets_A)\n",
    "ds_A = ds_A.swap_dims({'obs': 'time'})\n",
    "ds_A = ds_A.sortby('time')\n",
    "ds_B = xr.open_mfdataset(datasets_B)\n",
    "ds_B = ds_B.swap_dims({'obs': 'time'})\n",
    "ds_B = ds_B.sortby('time')\n",
    "#ds = ds.chunk({'time': 100})\n",
    "print(ds_A)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#print ds_B\n",
    "print ds_B.dims['bin']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Extract the east, north and west values along the time dimensions. Note the .T for two dimensional variables."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "time_A = ds_A['time'].data\n",
    "pressure_A = ds_A['pressure'].data\n",
    "east_A = ds_A['eastward_seawater_velocity'].data.T\n",
    "north_A = ds_A['northward_seawater_velocity'].data.T\n",
    "up_A = ds_A['upward_seawater_velocity'].data.T\n",
    "bad_beams_A=ds_A['percent_bad_beams'].data.T               "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "time_B = ds_B['time'].data\n",
    "pressure_B = ds_B['pressure'].data\n",
    "east_B = ds_B['eastward_seawater_velocity'].data.T\n",
    "north_B = ds_B['northward_seawater_velocity'].data.T\n",
    "up_B = ds_B['upward_seawater_velocity'].data.T\n",
    "bad_beams_B=ds_B['percent_bad_beams'].data.T  "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print len(time_B)\n",
    "print len(time_A)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Plotting and Analyzing the data\n",
    "Next we will plot the data."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To screen out surface noise velocities we will set the colorbar limit to the 90th percentile of the data."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "lim_east = float(\"%2.2f\" % np.nanpercentile(east_B, 90))\n",
    "lim_north = float(\"%2.2f\" % np.nanpercentile(north_B, 90))\n",
    "lim_up = float(\"%2.2f\" % np.nanpercentile(up_B, 90))\n",
    "u_v_w_B = max([lim_east, lim_north, lim_up])\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next code cell creates fake depth range, which is approximation of real depths of an instrument cell, which are usefull to the data evaluation. To be more accurate one must use the recorded depth of an instrument and calculate bin depth based on blanking distance from an instrument and cell size."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Mooring A\n",
    "ny, nx = ds_A.dims['bin'],len(time_A)# 55 - number of bin cells (can be different for differrent deployments) \n",
    "xmin, xmax = 0, len(time_A)\n",
    "ymin, ymax = -500, 0\n",
    "xi = np.linspace(xmin, xmax, nx)\n",
    "yi = np.linspace(ymin, ymax, ny)\n",
    "x_i, y_i = np.meshgrid(time_A, yi)\n",
    "bin_depths_A = np.asarray(y_i)\n",
    "print bin_depths_A.shape\n",
    "\n",
    "#Mooring B\n",
    "ny, nx = ds_B.dims['bin'],len(time_B)# 55 - number of bin cells (can be different for differrent deployments) \n",
    "xmin, xmax = 0, len(time_B)\n",
    "ymin, ymax = -500, 0\n",
    "xi = np.linspace(xmin, xmax, nx)\n",
    "yi = np.linspace(ymin, ymax, ny)\n",
    "x_i, y_i = np.meshgrid(time_B, yi)\n",
    "bin_depths_B = np.asarray(y_i)\n",
    "print bin_depths_B.shape"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "And plot (mooring A). Also below I created a function which can plot up to 4 ADCP parameters (e.g. U,V,W velocities, or Beam SNR - beam 1, 2, 3, 4)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_ADCP_data(*args):\n",
    "    num_of_vars=len(args)-2\n",
    "    plt.close()\n",
    "    if num_of_vars>4:\n",
    "      fig, axes = plt.subplots(num_of_vars-2, sharex=True)\n",
    "    else:\n",
    "      fig, axes = plt.subplots(num_of_vars, sharex=True)\n",
    "    fig.set_size_inches(20, 9)\n",
    "    if num_of_vars==1:\n",
    "      p = axes.pcolormesh(args[0], args[1], args[2], cmap='jet')\n",
    "      axes.set_ylim(-500,-100)\n",
    "      axes.set_title('% bad beams')\n",
    "      fig.colorbar(p,label='%')\n",
    "      plt.show()\n",
    "    if num_of_vars==3:\n",
    "      lim_east = float(\"%2.2f\" % np.nanpercentile(args[2], 90))\n",
    "      lim_north = float(\"%2.2f\" % np.nanpercentile(args[3], 90))\n",
    "      lim_up = float(\"%2.2f\" % np.nanpercentile(args[4], 90))\n",
    "      u_v_w = max([lim_east, lim_north, lim_up])\n",
    "      p0 = axes[0].pcolormesh(args[0], args[1], args[2], cmap='RdBu',vmin=-u_v_w,vmax=u_v_w)\n",
    "      axes[0].set_ylim(-500,-100)\n",
    "      axes[0].set_title('Eastward Velocity (u)')\n",
    "      p1 = axes[1].pcolormesh(args[0], args[1], args[3], cmap='RdBu',vmin=-u_v_w,vmax=u_v_w)\n",
    "      axes[1].set_title('Northward Velocity (v)')\n",
    "      axes[1].set_ylim(-500,-100)\n",
    "      axes[1].set_ylabel('Depth')\n",
    "      p2 = axes[2].pcolormesh(args[0], args[1], args[4], cmap='RdBu',vmin=-u_v_w,vmax=u_v_w)\n",
    "      axes[2].set_title('Upward Velocity (w)')\n",
    "      axes[2].set_ylim(-500,-100)\n",
    "      axes[2].set_xlabel('Time')\n",
    "      fig.colorbar(p0,ax=axes.ravel().tolist(), label='m/s')\n",
    "      plt.show()\n",
    "    if num_of_vars>=4:\n",
    "      p0 = axes[0].pcolormesh(args[0], args[1], args[2], cmap=args[6])\n",
    "      axes[0].set_ylim(-500,-100)\n",
    "      axes[0].set_title('Beam 1')\n",
    "      p1 = axes[1].pcolormesh(args[0], args[1], args[3], cmap=args[6])\n",
    "      axes[1].set_title('Beam 2')\n",
    "      axes[1].set_ylim(-500,-100)\n",
    "      axes[1].set_ylabel('Depth')\n",
    "      p2 = axes[2].pcolormesh(args[0], args[1], args[4], cmap=args[6])\n",
    "      axes[2].set_title('Beam 3')\n",
    "      axes[2].set_ylim(-500,-100)\n",
    "      p3 = axes[3].pcolormesh(args[0], args[1], args[5], cmap=args[6])\n",
    "      axes[3].set_title('Beam 4')\n",
    "      axes[3].set_ylim(-500,-100)\n",
    "      axes[3].set_xlabel('Time')\n",
    "      fig.colorbar(p0,ax=axes.ravel().tolist(), label=args[7])\n",
    "      plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The next plots show the East, North and Vertical velocity components for mooring A and mooring B.\n",
    "East and North components look coherent between two deployments. The vertical component might not be resolved by 75kHz ADCP, resulting in the pattern observed on the plots."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_ADCP_data(time_A,bin_depths_A,east_A,north_A,up_A)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_ADCP_data(time_B,bin_depths_B,east_B,north_B,up_B)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "time_B[0:100]\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next we compute correlation and spectrum between each beam between two instruments.\n",
    "For correlation we first interpolate data from one instrument onto timeframe of another instrument\n",
    "Then we fill NaN values and run a running mean of 100 hours on data.\n",
    "\n",
    "The correlation values do not show high values, but we can see from the timeseries plots that the data btween each instrument is pretty coherent. This is also confirmed by comparison of the Power Spectral Density (PSD) for datasets from two intruments. The PSD shapes are very close between each moorings and show a spike around 300 day period."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def fill_nan(A):\n",
    "    '''\n",
    "    interpolate to fill nan values\n",
    "    '''\n",
    "    inds = np.arange(A.shape[0])\n",
    "    good = np.where(np.isfinite(A))\n",
    "    f = interpolate.interp1d(inds[good], A[good],bounds_error=False)\n",
    "    B = np.where(np.isfinite(A),A,f(inds))\n",
    "    return B\n",
    "\n",
    "  \n",
    "def runningMeanFast(x, N):\n",
    "    return np.convolve(x, np.ones((N,))/N)[(N-1):]\n",
    "  \n",
    "  \n",
    "def plot_PSD_welch(y, fq, c,l):\n",
    "    from scipy import signal\n",
    "    plt.grid(True,which=\"both\",ls=\"-\", color='0.65')\n",
    "    f,PSD = signal.welch(y,float(fq))\n",
    "    x=np.linspace(1,len(y)/2+1,len(y)/2+1)\n",
    "    x=f**(-1)\n",
    "    x=x\n",
    "    plt.loglog(x,PSD, color=c, label=l)\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel('Period (days)')\n",
    "    plt.ylabel('Spectral Density')\n",
    "    #plt.gca().invert_xaxis()\n",
    "    #plt.show()\n",
    "    return x,PSD\n",
    "  \n",
    "  \n",
    "import scipy.interpolate as interpolate\n",
    "from numpy import nan\n",
    "ds_A.dims['bin']\n",
    "ds_A.dims['time']\n",
    "\n",
    "test_a=pd.to_numeric(ds_A['time'].values, errors='coerce')\n",
    "test_b=pd.to_numeric(ds_B['time'].values, errors='coerce')\n",
    "\n",
    "#check first 20 bins\n",
    "for i in range(10):\n",
    "\n",
    "  #print east_A.shape, time_A[0],time_B[0]\n",
    "  east_A_new=np.asarray(east_A[i,:])\n",
    "  #print time_B.values\n",
    "  #new_B=np.interp(test_b, test_a, east_A_new)\n",
    "  #print np.asarray(test_a[:])\n",
    "  interp_data = interpolate.interp1d(np.asarray(test_a), fill_nan(east_A_new),  kind='cubic',  bounds_error=None)#, fill_value=nan)\n",
    "  #interp_data = interpolate.interp1d(time_A, east_A[:,0], kind='cubic',  bounds_error=None)#, fill_value=nan)\n",
    "\n",
    "  new_dat = interp_data(np.asarray(test_b))\n",
    "  #print new_B.shape\n",
    "\n",
    "\n",
    "  #plt.plot(time_A)\n",
    "  fig=plt.figure()\n",
    "  plt.plot(time_B,runningMeanFast(fill_nan(east_B[i,:]),100))\n",
    "  plt.plot(time_B,runningMeanFast(fill_nan(new_dat),100))\n",
    "  r=np.corrcoef(runningMeanFast(fill_nan(east_B[i,:]),100), runningMeanFast(fill_nan(new_dat),100))\n",
    "  fig=plt.figure()\n",
    "  plot_PSD_welch(fill_nan(east_B[i,:]),float(24),'r', '1')\n",
    "  plot_PSD_welch(fill_nan(new_dat),float(24),'b', '2')\n",
    "  plt.gca().invert_xaxis()\n",
    "#slope, intercept, r_value, p_value, std_err = stats.linregress(east_B[1,:], new_dat)\n",
    "  print i,r[0,1]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next we check time series of pressure data from two instruments. We can observe that the pressure time series are very coherent between mooring A and mooring B ADCPs"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#check pressure between two instruments\n",
    "plt.plot(time_A,pressure_A/1000)\n",
    "plt.plot(time_B,pressure_B/1000)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next we are interested in checking the signal quality parameters of ADCP data. First two plots show the bad beam data for each deployment. We can observe that the closer to the surface the data are the higher amount of bad beams dataset contains"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_ADCP_data(time_B,bin_depths_B,bad_beams_B)\n",
    "plot_ADCP_data(time_A,bin_depths_A,bad_beams_A)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The similar is relevant to beam signal correlation and beam echo inensity, as the data within the beam is rejected based on these two parameters. However sometimes user might want to use different rejection treshold, thus it is useful to analyze plots of these two parameters separately for each beam."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cor_1_A = ds_A['correlation_magnitude_beam1'].data.T\n",
    "cor_2_A = ds_A['correlation_magnitude_beam2'].data.T\n",
    "cor_3_A = ds_A['correlation_magnitude_beam3'].data.T\n",
    "cor_4_A = ds_A['correlation_magnitude_beam4'].data.T\n",
    "\n",
    "cor_1_B = ds_B['correlation_magnitude_beam1'].data.T\n",
    "cor_2_B = ds_B['correlation_magnitude_beam2'].data.T\n",
    "cor_3_B = ds_B['correlation_magnitude_beam3'].data.T\n",
    "cor_4_B = ds_B['correlation_magnitude_beam4'].data.T\n",
    "\n",
    "echo_1_A = ds_A['echo_intensity_beam1'].data.T\n",
    "echo_2_A = ds_A['echo_intensity_beam2'].data.T\n",
    "echo_3_A = ds_A['echo_intensity_beam3'].data.T\n",
    "echo_4_A = ds_A['echo_intensity_beam4'].data.T\n",
    "\n",
    "echo_1_B = ds_B['echo_intensity_beam1'].data.T\n",
    "echo_2_B = ds_B['echo_intensity_beam2'].data.T\n",
    "echo_3_B = ds_B['echo_intensity_beam3'].data.T\n",
    "echo_4_B = ds_B['echo_intensity_beam4'].data.T\n",
    "\n",
    "\n",
    "plot_ADCP_data(time_A,bin_depths_A,cor_1_A,cor_2_A,cor_3_A,cor_4_A, 'gist_yarg', 'signal correlation %')\n",
    "plot_ADCP_data(time_B,bin_depths_B,cor_1_B,cor_2_B,cor_3_B,cor_4_B, 'gist_heat','beam echo intensity (counts)')\n",
    "\n",
    "plot_ADCP_data(time_A,bin_depths_A,echo_1_A,echo_2_A,echo_3_A,echo_4_A, 'gist_yarg', 'signal correlation %')\n",
    "plot_ADCP_data(time_B,bin_depths_B,echo_1_B,echo_2_B,echo_3_B,echo_4_B, 'gist_heat','beam echo intensity (counts)')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6.Conclusions\n",
    "\n",
    "Based on this (somewhat cursory) analysis of the Irminger Flanking Mooring A and Mooring B ADCP data, we note the following takeaways:\n",
    "* In general, the east and north velocity components during deployment look largely reasonable.  There are no major outliers.\n",
    "* The data seem to align between two instruments. However there should be precaution taken into account, when using the data:\n",
    "  * Vertical velocity fluctuations seem to be unresolved by ADCP data.\n",
    "  * The low sampling rate of the profilers\n",
    "  * Additional verification of velocity data using cruise data would be benefitial\n",
    "  \n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
