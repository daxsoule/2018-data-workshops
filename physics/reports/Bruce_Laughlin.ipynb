{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Bruce2.ipynb",
   "version": "0.3.2",
   "views": {},
   "default_view": {},
   "provenance": [
    {
     "file_id": "1iy05EQrhYEB5groJaooDOV_WdMaSFh68",
     "timestamp": 1527169777424
    },
    {
     "file_id": "1SnJQ-uQpWtXoeaPCbKfe6yvzPenum6gR",
     "timestamp": 1527101790270
    },
    {
     "file_id": "1783o_5T7i9x9viaSGD-5hhSMmhimdOyG",
     "timestamp": 1523618562907
    }
   ],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python2",
   "display_name": "Python 2"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Pioneer Central Inshore Profiler (CP02PMCI) Data Quality Report\n",
    "**Evaluation Date**: 5/23/2018\n",
    "\n",
    "**Evaluator**: Bruce Laughlin\n",
    "\n",
    "## Review Summary\n",
    "This report summarizes a data quality review of the Pioneer Central Inshore Profiler (CP02PMCI).  We focus on data from the summer of 2017 (Deployment 8), and compare the profiler's CTD with a cruise CTD "
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Selected instruments included in this report\n",
    "In this report, we will evaluate the CTD (specifically temperature and salinity) from the Pioneer Central Inshore Profiler (CP02PMCI), focusing on the summer of 2017 (Deployment 8).\n",
    "\n",
    "We focus on the telmetered data stream in this review because the recovered data are not yet available in the system for Deployment 8 (July 2017 and beyond). A graph of data availability is available at http://ooi.visualocean.net/instruments/stats-monthly/CP02PMCI-WFP01-03-CTDPFK000."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Setup Instrument Variables\n",
    "site = 'CP02PMCI'\n",
    "node = 'WFP01'\n",
    "instrument = '03-CTDPFK000'\n",
    "method = 'telemetered'\n",
    "stream = 'ctdpf_ckl_wfp_instrument'\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Time periods of interest\n",
    "We will focus on the following time periods for evaluation:\n",
    "\n",
    "\n",
    "* June 16, 2017 to November 11, 2017 - All of Deployment 8\n",
    "* One week around November 11, 2017 - The changeover from Deployment 8 to 9"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Related Metadata\n",
    "In this section, we will review some of metadata available in the system to make sure it is present and correct.\n",
    "\n",
    "Before we get started, we need to set up our Python environment with some libraries, variables and functions we will need later in this report."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Setup the Python processing environment \n",
    "import requests\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import time\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# API Information\n",
    "USERNAME ='OOIAPI-D7TI8MK8MAL1PZ'\n",
    "TOKEN= 'J6UF0R8V58I215'\n",
    "DATA_API = 'https://ooinet.oceanobservatories.org/api/m2m/12576/sensor/inv'\n",
    "VOCAB_API = 'https://ooinet.oceanobservatories.org/api/m2m/12586/vocab/inv'\n",
    "ASSET_API = 'https://ooinet.oceanobservatories.org/api/m2m/12587'\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Specify some functions to convert timestamps\n",
    "ntp_epoch = datetime.datetime(1900, 1, 1)\n",
    "unix_epoch = datetime.datetime(1970, 1, 1)\n",
    "ntp_delta = (unix_epoch - ntp_epoch).total_seconds()\n",
    "\n",
    "def ntp_seconds_to_datetime(ntp_seconds):\n",
    "    return datetime.datetime.utcfromtimestamp(ntp_seconds - ntp_delta).replace(microsecond=0)\n",
    "  \n",
    "def convert_time(ms):\n",
    "  if ms != None:\n",
    "    return datetime.datetime.utcfromtimestamp(ms/1000)\n",
    "  else:\n",
    "    return None\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3a. Vocabulary Metadata\n",
    "First, let's grab the basic vocabulary information (metadata) from the system to make sure we have the right instrument."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Setup the API request url\n",
    "data_request_url ='/'.join((VOCAB_API,site,node,instrument))\n",
    "print data_request_url\n",
    "\n",
    "# Grab the information from the server\n",
    "r = requests.get(data_request_url, auth=(USERNAME, TOKEN))\n",
    "data = r.json()\n",
    "data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "All this looks good!"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3b. Deployment Information\n",
    "Next, let's grab some information about the deployments for this instrument.  We will grab all of the deployments available in the system for 2017 and then output the date ranges, latitude/longitude, asset ID, and sensor ID for each.  Note that the **reference designator** specified above represents the geographical location of an instrument across all deployments (e.g. the CTD on the Pioneer Central Inshore Profiler), the **Sensor ID** (and its Asset ID equivalent) represent the specific instrument used for a given deployment (i.e. a unique make, model, and serial numbered instrument)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Setup the API request url\n",
    "data_request_url = ASSET_API + '/events/deployment/query'\n",
    "params = {\n",
    "  'beginDT':'2017-01-01T00:00:00.000Z',\n",
    "  'endDT':'2018-01-01T00:00:00.000Z',\n",
    "  'refdes':site+'-'+node+'-'+instrument,   \n",
    "}\n",
    "\n",
    "# Grab the information from the server\n",
    "r = requests.get(data_request_url, params=params, auth=(USERNAME, TOKEN))\n",
    "data = r.json()\n",
    "\n",
    "df = pd.DataFrame() # Setup empty array\n",
    "for d in data:\n",
    "  df = df.append({\n",
    "      'deployment': d['deploymentNumber'],\n",
    "      'start': convert_time(d['eventStartTime']),\n",
    "      'stop': convert_time(d['eventStopTime']),\n",
    "      'latitude': d['location']['latitude'],\n",
    "      'longitude': d['location']['longitude'],\n",
    "      'sensor': d['sensor']['uid'],\n",
    "      'asset_id': d['sensor']['assetId'],\n",
    "    }, ignore_index=True)\n",
    "df"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3c. Calibration Information\n",
    "When the system delivers data, it often uses a number of calibration coefficients to generated derived data products.\n",
    "\n",
    "It turns out that this CTD instrument doesn't have any calibration information, because none is required by the system.\n",
    "\n",
    "Instead, as an example of what the calibration information might look like (and to make sure something has been added to the system for this deployment), let's pull the calibration information for the neighboring FLORT for Deployment 8.  \n",
    "\n",
    "Because the asset API can return confusing results, we will limit the search to just one day within the deployment time range."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Setup the API request url\n",
    "data_request_url = ASSET_API + '/asset/cal'\n",
    "params = {\n",
    "  'beginDT':'2017-07-01T00:00:00.000Z',\n",
    "  'endDT':'2017-07-02T00:00:00.000Z',\n",
    "  'refdes':site+'-'+node+'-'+'04-FLORTK000',\n",
    "}\n",
    "\n",
    "# Grab the information from the server\n",
    "r = requests.get(data_request_url, params=params, auth=(USERNAME, TOKEN))\n",
    "data = r.json()\n",
    "\n",
    "# Reformat the data into a pretty table\n",
    "df = pd.DataFrame() # Setup empty array\n",
    "for d in data:\n",
    "  for dd in d['sensor']['calibration']:\n",
    "    for ddd in dd['calData']:\n",
    "      df = df.append({\n",
    "        'value': ddd['value'],\n",
    "        'start': convert_time(ddd['eventStartTime']),\n",
    "        'stop': convert_time(ddd['eventStopTime']),\n",
    "        'name': ddd['eventName'],\n",
    "        'assetUid': ddd['assetUid'],\n",
    "        }, ignore_index=True)\n",
    "    \n",
    "df = df.sort_values(by=['start','name'])\n",
    "df\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "From the deployment table above, we can see that Deployment 8 began on 6/16/2017.  The system uses the calibration values that are closest in time but before the deployment start date. Note also, that calibrations do not have an end date.  It is assumed they are valid until the next calibration for the specific asset. \n",
    "\n",
    "So in this case, for Deployment 8, the calibration values from 12/12/2016 were used, which was when the instrument was last calibrated.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3d. Annotations\n",
    "Finally, let's pull any relevant annotations for the CTD instrument."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ANNO_API = 'https://ooinet.oceanobservatories.org/api/m2m/12580/anno/find'\n",
    "params = {\n",
    "  'beginDT':int(datetime.date(2017,1,1).strftime('%s'))*1000,\n",
    "  'endDT':int(datetime.date(2018,1,1).strftime('%s'))*1000,\n",
    "  'refdes':site+'-'+node+'-'+instrument,\n",
    "}\n",
    "\n",
    "r = requests.get(ANNO_API, params=params, auth=(USERNAME, TOKEN))\n",
    "data = r.json()\n",
    "\n",
    "df = pd.DataFrame() # Setup empty array\n",
    "for d in data:\n",
    "  df = df.append({\n",
    "    'annotation': d['annotation'],\n",
    "    'start': convert_time(d['beginDT']),\n",
    "    'stop': convert_time(d['endDT']),\n",
    "    'site': d['subsite'],\n",
    "    'node': d['node'],\n",
    "    'sensor': d['sensor'],\n",
    "    'id': d['id']\n",
    "  }, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', -1) # Show the full annotation text\n",
    "df"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "There's only one annotation currently in the system, and it's for the entire Wire Following Profiler. Apparently, transmission of telemetered data ceased approximately 4 months into deployment 7, and transmission did not begin again until the beginning of deployment 8.  Recovered data is available for this period.  There are currently no annotations for deployments 8 and 9; we will explore deplyoment 8 in order to confirm that everything is in order."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. The full dataset\n",
    "Now let's take a look at a large range of data.  We shall look at all of Deployment 8.  First we need to add some additional libraries to our Python environment."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "!pip install netCDF4\n",
    "import netCDF4 as nc\n",
    "\n",
    "!pip install xarray\n",
    "import xarray as xr\n",
    "\n",
    "!pip install cmocean\n",
    "import cmocean\n",
    "\n",
    "!pip install dask\n",
    "!pip install bokeh\n",
    "\n",
    "import re\n",
    "import os\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For simplicity, we used the Data Portal to make a download request for all available data.  After receiving the email, we looked through the results and here we specify just the netCDF file that includes the CTD data for all of deployment 8.  Using this data file, we will create a few different plots to better understand the available data."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# DATA REQUEST!\n",
    "# Here try to look at entire year\n",
    "\n",
    "beginDT = '2017-01-01T00:00:00.000Z',\n",
    "endDT = '2018-01-01T00:00:00.000Z',\n",
    "\n",
    "base_url = 'https://ooinet.oceanobservatories.org/api/m2m/12576/sensor/inv/'\n",
    "\n",
    "data_request_url ='/'.join((base_url,site,node,instrument,method,stream))\n",
    "params = {\n",
    "    'beginDT':beginDT,\n",
    "    'endDT':endDT,   \n",
    "}\n",
    "r = requests.get(data_request_url, params=params, auth=(USERNAME,TOKEN))\n",
    "data = r.json()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data['allURLs'][0]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%time\n",
    "check_complete = data['allURLs'][1] + '/status.txt'\n",
    "for i in range(1800): \n",
    "    r = requests.get(check_complete)\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        print('request completed')\n",
    "        break\n",
    "    else:\n",
    "        time.sleep(1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Parse the thredds server to get a list of all NetCDF files. Each deployment is seperated into a seperate netcdf file"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://opendap.oceanobservatories.org/thredds/catalog/ooi/blaughli@ucsc.edu/20180524T135952-CP02PMCI-WFP01-03-CTDPFK000-telemetered-ctdpf_ckl_wfp_instrument/catalog.html'\n",
    "#url = data['allURLs'][0]\n",
    "#url\n",
    "tds_url = 'https://opendap.oceanobservatories.org/thredds/dodsC'\n",
    "datasets = requests.get(url).text\n",
    "urls = re.findall(r'href=[\\'\"]?([^\\'\" >]+)', datasets)\n",
    "x = re.findall(r'(ooi/.*?.nc)', datasets)\n",
    "for i in x:\n",
    "    if i.endswith('.nc') == False:\n",
    "        x.remove(i)\n",
    "for i in x:\n",
    "    try:\n",
    "        float(i[-4])\n",
    "    except:\n",
    "        x.remove(i)\n",
    "datasets = [os.path.join(tds_url, i) for i in x]\n",
    "datasets"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "https://opendap.oceanobservatories.org/thredds/dodsC/ooi/blaughli@ucsc.edu/20180524T135952-CP02PMCI-WFP01-03-CTDPFK000-telemetered-ctdpf_ckl_wfp_instrument/deployment0009_CP02PMCI-WFP01-03-CTDPFK000-telemetered-ctdpf_ckl_wfp_instrument_20171107T230003-20171231T223906.983240.nc"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Load all files into a single xarray dataset."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# see notes on xarray - it is a really smart and powerful tool for dealing with data\n",
    "# loads data to memory, not requiring download.  but actually loads the structure of the data\n",
    "# rather than needing to load it all at once.  very smart, look up documentation (github, google)\n",
    "ds = xr.open_mfdataset(datasets)\n",
    "ds = ds.swap_dims({'obs': 'time'})\n",
    "#ds = ds.chunk({'time': 100}) # Sage - \"Friedrich likes this 'ds.chunk' thing\"\n",
    "#ds = ds.sortby('time') # data from different deployments can overlap so we want to sort all data by time stamp.\n",
    "ds\n",
    "#ds['ctdmo_seawater_pressure'] #explore specific variable attributes\n",
    "#ds['ctdmo_seawater_pressure'].comment #explore specific attribute of specific variable ( \"comment\" here )\n",
    "# ['ctdmo_seawater_pressure']. then \"tab\" will give you all options (attributes AND functions)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# the following attempt to look at ALL of 2017 was too demanding on both my own computer's memory and what Google Collab allotted me, apparently\n",
    "\n",
    "#data_url_7 = 'https://opendap.oceanobservatories.org/thredds/dodsC/ooi/blaughli@ucsc.edu/20180524T135952-CP02PMCI-WFP01-03-CTDPFK000-telemetered-ctdpf_ckl_wfp_instrument/deployment0007_CP02PMCI-WFP01-03-CTDPFK000-telemetered-ctdpf_ckl_wfp_instrument_20170101T000001-20170207T120924.985611.nc'\n",
    "#data_url_8 = 'https://opendap.oceanobservatories.org/thredds/dodsC/ooi/blaughli@ucsc.edu/20180524T135952-CP02PMCI-WFP01-03-CTDPFK000-telemetered-ctdpf_ckl_wfp_instrument/deployment0008_CP02PMCI-WFP01-03-CTDPFK000-telemetered-ctdpf_ckl_wfp_instrument_20170617T030003-20171026T134053.987597.nc'\n",
    "#data_url_9 = 'https://opendap.oceanobservatories.org/thredds/dodsC/ooi/blaughli@ucsc.edu/20180524T135952-CP02PMCI-WFP01-03-CTDPFK000-telemetered-ctdpf_ckl_wfp_instrument/deployment0009_CP02PMCI-WFP01-03-CTDPFK000-telemetered-ctdpf_ckl_wfp_instrument_20171107T230003-20171231T223906.983240.nc'\n",
    "\n",
    "# Open the dataset\n",
    "#ds7 = xr.open_dataset(data_url_7)\n",
    "#ds8 = xr.open_dataset(data_url_8)\n",
    "#ds9 = xr.open_dataset(data_url_9)\n",
    "\n",
    "\n",
    "# Swap the dimensions\n",
    "#ds7 = ds7.swap_dims({'obs': 'time'})\n",
    "#ds8 = ds8.swap_dims({'obs': 'time'})\n",
    "#ds9 = ds9.swap_dims({'obs': 'time'})\n",
    "\n",
    "#ds8\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_url = 'https://opendap.oceanobservatories.org/thredds/dodsC/ooi/blaughli@ucsc.edu/20180524T191503-CP02PMCI-WFP01-03-CTDPFK000-telemetered-ctdpf_ckl_wfp_instrument/deployment0008_CP02PMCI-WFP01-03-CTDPFK000-telemetered-ctdpf_ckl_wfp_instrument_20170617T030003-20171026T134053.987597.nc'\n",
    "# data_url = 'https://opendap.oceanobservatories.org/thredds/dodsC/ooi/blaughli@ucsc.edu/20180524T191503-CP02PMCI-WFP01-03-CTDPFK000-telemetered-ctdpf_ckl_wfp_instrument/deployment0009_CP02PMCI-WFP01-03-CTDPFK000-telemetered-ctdpf_ckl_wfp_instrument_20171107T230003-20180101T000100.902724.nc'\n",
    "\n",
    "# Open the dataset\n",
    "ds = xr.open_dataset(data_url)\n",
    "\n",
    "\n",
    "# Swap the dimensions\n",
    "ds = ds.swap_dims({'obs': 'time'})\n",
    "\n",
    "ds"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ds['time']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let's create a quick timeseries plot of pressure.  If we plot the entire dataset, it will just be a blue blob, so let's just plot the first 20K points to see how the profiler works."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#ds['ctdpf_ckl_seawater_pressure'][0:100000].plot()\n",
    "ds['ctdpf_ckl_seawater_pressure'][0:20000].plot()\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "It looks like during this time period the profiler regularly visited the depths of approximately 100, 90, and 25 in repeating order.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As a rough check on the consistency of this depth profile over the entire deployment, let's plot a timeseries of pressure over the whole dataset.  We should expect to see a solid block of blue, representing regular behavior as we saw in the previous plot carried out over the entire deployment."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ds['ctdpf_ckl_seawater_pressure'].plot();"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "It appears as though the profiler did not get stuck at any point during this deployment, and that it consistently traveled between its initial depth endpoints.  Now, let's take a look at the full timeseries of temperature.  "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ds['ctdpf_ckl_seawater_temperature'].plot();"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This is not the most useful plot, as the full depth range of the profiler is also included, but it does provide a general sense of the data.  The temperature \"envelope\" has a smaller range in the early summer (between 7.5 and 17.5 degrees), which narrows in July before widening considerably in the early fall.  As fall progresses, the temperature range evolves from spanning 11 and 22.5 degrees to spanning 12.5 and 21 degrees through the end of the deployment in early November.  While sea surface temperature in the region typically peaks in July and August before steadily decreasing into winter, satellite images show regular gulf stream impingement in the area via rings and filaments through early November (2017 images in order: July 7, August 23, September 5, October 4, November 3 2017)\n",
    "\n",
    "![Sea Surface Temperature for 07/08/2017](https://marine.rutgers.edu/cool/regions/bigbight/sst_comp/noaa/2017/img/170708.189.comp.lnt.jpg =x300)\n",
    "\n",
    "![Sea Surface Temperature for 08/23/2017](https://marine.rutgers.edu/cool/regions/sw06/sst/noaa/2017/img/170823.235.0012.n18.jpg =x300)\n",
    "\n",
    "![Sea Surface Temperature for 09/05/2017](https://marine.rutgers.edu/cool/regions/bigbight/sst_comp/noaa/2017/img/170905.248.comp.lnt.jpg =x300)\n",
    "\n",
    "![Sea Surface Temperature for 10/04/2017](https://marine.rutgers.edu/cool/regions/bigbight/sst_comp/noaa/2017/img/171004.277.comp.lnt.jpg =x300)\n",
    "\n",
    "![Sea Surface Temperature for 11/3/2017](https://marine.rutgers.edu/cool/regions/bigbight/sst_comp/noaa/2017/img/171103.307.comp.lnt.jpg =x300)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next, let's create some histograms to see the full range of data.  (Thanks to xarray, this is easy to do in just one line!)  Histograms aren't really ideal for understanding a profiler dataset since all depths are included, but let's see what they looks like anyway."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(16,6))\n",
    "ds['ctdpf_ckl_seawater_temperature'].plot.hist(bins=100, ax=ax1)\n",
    "ds['practical_salinity'].plot.hist(bins=100, ax=ax2);\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "At first glance, the data doesn't appear to include any significant outliers.  However, the salinity histogram appears to have bins for salinity values as high as 39 psu, so we may actually have small numbers of outliers there.  As to whether the times of warm temperature or relatively fresher waters are valid, we'd have to look at the dataset more closely, e.g. specific depths or time periods.\n",
    "\n",
    "So, let's create a profile timeseries to better understand the full dataset."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dtime = ds['time'].values\n",
    "pressure = ds['ctdpf_ckl_seawater_pressure'].values\n",
    "temperature = ds['ctdpf_ckl_seawater_temperature'].values\n",
    "salinity = ds['practical_salinity'].values\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, (ax1,ax2) = plt.subplots(2,1, sharex=True, sharey=True, figsize=(16,8))\n",
    "sc1 = ax1.scatter(dtime, pressure, c=temperature, cmap='RdYlBu_r', s=2)\n",
    "sc2 = ax2.scatter(dtime, pressure, c=salinity, cmap='GnBu', s=2)\n",
    "\n",
    "# Because the X and Y axes are shared, we only have to set limits once\n",
    "ax1.invert_yaxis() # Invert y axis\n",
    "ax1.set_xlim(dtime[0],dtime[-1]) # Set the time limits to match the dataset\n",
    "\n",
    "cbar = fig.colorbar(sc1, ax=ax1, orientation='vertical', extend='both')\n",
    "cbar.ax.set_ylabel('Temperature ($^\\circ$C)')\n",
    "cbar = fig.colorbar(sc2, ax=ax2, orientation='vertical', extend='both')\n",
    "cbar.ax.set_ylabel('Salinity')\n",
    "\n",
    "ax1.set_ylabel('Pressure (db)')\n",
    "ax2.set_ylabel('Pressure (db)')\n",
    "\n",
    "fig.suptitle('CP02PMCI Deployment 9')\n",
    "fig.subplots_adjust(top=0.95);\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Comparison of temperature and salinity profile timeseries for the entire deployment show that there is a strong correlation between temperature and salinity in the profiles.  Early in the deployment, during summer, low temperature values correspond to low salinity values, suggesting that cool, fresh labrador current water is dominating the water column sampled by the profiler.  The \"anomalously\" warm temperatures observed in the fall and early winter correspond to peaks in salinity, suggesting the impingement of gulf stream water onto the shelf.  The scale on the salinity plot indicates that there is an anomalous vaue near 39 psu, and we see a small dark patch around August 8, 2017 at around 40db pressure.  This extremely high salinity value was represented in the histogram of the data, and it warrants a deeper investigation that we'll carry out in the next section."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. A closer look\n",
    "Before looking at the period of crossover between deployments, we should explore the anomolous salinity values observed previously.  In step 4, we used a netCDF file.  For this step, we will grab the data directly from the API using a synchronous request, which is limited to 20K datapoints.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Setup request for instrument data\n",
    "data_request_url ='/'.join((DATA_API,site,node,instrument,method,stream))\n",
    "\n",
    "params = {\n",
    "  'beginDT':'2017-08-09T00:00:00.000Z',\n",
    "  'endDT':'2017-08-12T00:00:00.000Z',\n",
    "  'limit':20000,\n",
    "}\n",
    "\n",
    "# Grab the data\n",
    "r = requests.get(data_request_url, params=params, auth=(USERNAME, TOKEN))\n",
    "data = r.json()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Process the returned JSON dataset into something we can work with\n",
    "p_time = []\n",
    "p_temp = []\n",
    "p_sal = []\n",
    "p_pr = []\n",
    "for i in range(len(data)):\n",
    "  p_time.append(ntp_seconds_to_datetime(data[i]['time']))\n",
    "  p_temp.append(data[i]['ctdpf_ckl_seawater_temperature'])\n",
    "  p_sal.append(data[i]['practical_salinity'])\n",
    "  p_pr.append(data[i]['ctdpf_ckl_seawater_pressure'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# First, let's make a timeseries of pressure\n",
    "plt.plot_date(p_time,p_pr,'r.')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Pressure (db)')\n",
    "plt.title('Pressure data from CP02PMUO');"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we thought we observed earlier, the profiler appears to have behaved normally in its climbing pattern during our current period of interest."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.hist(p_sal)\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Some experimentation with matplotlib's hist function led to the above histogram, which indicates that there are at least 3 individual data points that fall above a salinity value of 36psu.  Let's see what a scatter plot looks like."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.scatter(p_time,p_sal)\n",
    "plt.xlim(datetime.datetime(2017, 8, 9, 00, 00, 00),datetime.datetime(2017, 8,12, 00, 00, 00))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Playing with matplotlib's \"scatter\" function led to this scatter plot focusing on the time range containing the anomalous data points, which appears to indicate that all of the outliers were recorded on the same day.   On the x-axis we have time, with 8-10-12 signifying August 10th at 1200 hours.  Let's zoom further in."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.scatter(p_time,p_sal)\n",
    "plt.xlim(datetime.datetime(2017, 8, 10, 10, 39, 39),datetime.datetime(2017, 8,11, 04, 39, 39))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let's zoom a bit futher in, focusing on what appear to be individual profiles"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.scatter(p_time,p_sal)\n",
    "plt.xlim(datetime.datetime(2017, 8, 10, 14, 39, 39),datetime.datetime(2017, 8,10, 18, 39, 39))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Ok, one more zoom!"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.scatter(p_time,p_sal)\n",
    "plt.xlim(datetime.datetime(2017, 8, 10, 16, 29, 1),datetime.datetime(2017, 8,10, 16, 41, 1))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This looks strange.  Let's compare to the previous profile."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.scatter(p_time,p_sal)\n",
    "plt.xlim(datetime.datetime(2017, 8, 10, 14, 58, 1),datetime.datetime(2017, 8,10, 15, 10, 1))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Comparing these scatter plots of single profiles, it appears as though the ctd recorded six consecutive outlier salinity values in the \"anomalous\" profile.  Now let's check the depths for the anomalous profile and a \"normal\" profile. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.scatter(p_time,p_pr)\n",
    "plt.xlim(datetime.datetime(2017, 8, 10, 16, 29, 1),datetime.datetime(2017, 8,10, 16, 41, 1))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.scatter(p_time,p_pr)\n",
    "plt.xlim(datetime.datetime(2017, 8, 10, 14, 58, 1),datetime.datetime(2017, 8,10, 15, 10, 1))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Both depth profiles look normal.  We also saw from the full data profile plot and the focused scatter plot above that the outlier values were recorded near the surface.  How do the temperature profiles look?  "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.scatter(p_time,p_temp)\n",
    "plt.xlim(datetime.datetime(2017, 8, 10, 16, 29, 1),datetime.datetime(2017, 8,10, 16, 41, 1))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.scatter(p_time,p_temp)\n",
    "plt.xlim(datetime.datetime(2017, 8, 10, 14, 58, 1),datetime.datetime(2017, 8,10, 15, 10, 1))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "These mirror each other, indicating consistency in the ctd's temperature measurements.  Now let's look at the previous descending profile, since the anomalous profile was a descending profile.  We'll re-plot the anomalous profile below it for comparison."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.scatter(p_time,p_sal)\n",
    "plt.xlim(datetime.datetime(2017, 8, 10, 13, 29, 1),datetime.datetime(2017, 8,10, 13, 40, 1))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.scatter(p_time,p_sal)\n",
    "plt.xlim(datetime.datetime(2017, 8, 10, 16, 29, 1),datetime.datetime(2017, 8,10, 16, 41, 1))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "There is obviously something going during this specific profile.  I would flag this and recommend that the data team contact the engineering team in charge of this deployment and notify them of a potential transient issue with the ctd on CP02PMCI."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now let's look at one week around the switchover between Deployments 7 and 8.  We'll explore what happens when two deployments overlap in time; in this case, deployment 7 ended on 6/17/2017 and deployment 8 began on 6/16/2017. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Setup request for instrument data\n",
    "data_request_url ='/'.join((DATA_API,site,node,instrument,method,stream))\n",
    "\n",
    "params = {\n",
    "  'beginDT':'2017-06-10T00:00:00.000Z',\n",
    "  'endDT':'2017-06-24T00:00:00.000Z',\n",
    "  'limit':10000,\n",
    "}\n",
    "\n",
    "# Grab the data\n",
    "r = requests.get(data_request_url, params=params, auth=(USERNAME, TOKEN))\n",
    "data = r.json()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Process the returned JSON dataset into something we can work with\n",
    "p_time = []\n",
    "p_temp = []\n",
    "p_sal = []\n",
    "p_pr = []\n",
    "for i in range(len(data)):\n",
    "  p_time.append(ntp_seconds_to_datetime(data[i]['time']))\n",
    "  p_temp.append(data[i]['ctdpf_ckl_seawater_temperature'])\n",
    "  p_sal.append(data[i]['practical_salinity'])\n",
    "  p_pr.append(data[i]['ctdpf_ckl_seawater_pressure'])\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# check if there are overlapping time values\n",
    "plt.plot(p_time)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# First, let's make a timeseries of pressure\n",
    "\n",
    "plt.plot_date(p_time,p_pr,'r.')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Pressure (db)')\n",
    "plt.title('Pressure data from CP02PMUO');\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "I struggled for a while but could NOT seem to be able to retrieve data from deployment 7 (which should have included the dates 6/10/2017 - 6/17/2018) in my synchronous request.  Please advise!"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Profile plots of temperature and salinity\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "\n",
    "# Create a color map to divide the two deployments\n",
    "kcolors = ['red' if t<datetime.datetime(2017,06,17,12,12,00) else 'blue' for t in p_time]\n",
    "#2017-06-17 12:12:00 \n",
    "\n",
    "ax1.scatter(p_temp,p_pr, c=kcolors, s=8)\n",
    "# ax1.plot(p_temp,p_pr,'r.')\n",
    "ax1.set_xlabel('Temperature (C)')\n",
    "ax1.set_ylabel('Pressure (dm)')\n",
    "# ax1.invert_yaxis()\n",
    "ax1.set_ylim(150,0)\n",
    "\n",
    "ax2.scatter(p_sal,p_pr, c=kcolors, s=8)\n",
    "# ax2.plot(p_sal,p_pr,'r.',label='Pioneer Profiler')\n",
    "ax2.set_xlabel('Salinity (psu)')\n",
    "fig.suptitle('CP02PMCI between June 10 and 24, 2017')\n",
    "fig.subplots_adjust(top=0.9);\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "I don't trust that the red dots are from deployment 7, since I don't seem to have that data in my pressure profile timeseries."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Comparison with a shipboard CTD\n",
    "Finally, we compare the profiler's CTD with data collected during the Pioneer 9 Cruise, Leg 3, Cast #4, which occurred on 2017-11-07 1642 UTC.  Note that, surprisingly to me, there is no telemetered data for deployment 8 past October 18, 2017, so this comparison is more for practice than for rigourous analysis.\n",
    "\n",
    "The location of the raw ascii data file for the CTD cast can be found below. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# First, specify the data file for the cruise CTD cast\n",
    "# Downcast\n",
    "# cruise_data_file = 'https://alfresco.oceanobservatories.org/alfresco/d/d/workspace/SpacesStore/ef3f532b-7570-43d9-b016-6b58c4429b15/dar24011.asc'\n",
    "# Down and Up Casts\n",
    "#cruise_data_file = 'https://alfresco.oceanobservatories.org/alfresco/d/d/workspace/SpacesStore/0ddd2680-e35d-46bc-ac1a-d350da4f409d/ar24011.asc'\n",
    "cruise_data_file = 'https://alfresco.oceanobservatories.org/alfresco/d/d/workspace/SpacesStore/32f88c36-87aa-4dd5-aa36-65ef102c182d/ar18c003.asc'\n",
    "\n",
    "\n",
    "# Read in the data file without headers due to a bug in the file (two of the header names run together)\n",
    "cruise_data = pd.read_table(cruise_data_file, delim_whitespace=True, header=None, skiprows=1)\n",
    "\n",
    "# Add back the necessary headers\n",
    "cruise_data = cruise_data.rename(columns={0:'Pressure', 1:'Temperature', 13:'Salinity'})\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Next let's grab the Profiler data using a synchronous request \n",
    "data_request_url ='/'.join((DATA_API,site,node,instrument,method,stream))\n",
    "\n",
    "params = {\n",
    "  'beginDT':'2017-10-17T00:00:00.000Z',\n",
    "  'endDT':'2017-10-18T00:00:00.000Z',\n",
    "  'limit':1000,\n",
    "}\n",
    "\n",
    "# Grab the data\n",
    "r = requests.get(data_request_url, params=params, auth=(USERNAME, TOKEN))\n",
    "data = r.json()\n",
    "#data\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Process the returned JSON dataset into something we can work with\n",
    "p_time = []\n",
    "p_temp = []\n",
    "p_sal = []\n",
    "p_pr = []\n",
    "for i in range(len(data)):\n",
    "  p_time.append(ntp_seconds_to_datetime(data[i]['time']))\n",
    "  p_temp.append(data[i]['ctdpf_ckl_seawater_temperature'])\n",
    "  p_sal.append(data[i]['practical_salinity'])\n",
    "  p_pr.append(data[i]['ctdpf_ckl_seawater_pressure'])\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Now let's plot the Cruise and Profiler data together\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "\n",
    "ax1.plot(cruise_data.Temperature,cruise_data.Pressure,'b')\n",
    "ax1.plot(p_temp,p_pr,'r.')\n",
    "ax1.set_xlabel('Temperature (C)')\n",
    "ax1.set_ylabel('Pressure (dm)')\n",
    "# ax1.invert_yaxis()\n",
    "ax1.set_ylim(150,0)\n",
    "\n",
    "ax2.plot(cruise_data.Salinity,cruise_data.Pressure,'b',label='Cruise CTD')\n",
    "ax2.plot(p_sal,p_pr,'r.',label='Pioneer Profiler')\n",
    "ax2.set_xlabel('Salinity (psu)')\n",
    "\n",
    "fig.suptitle('CP02PMCI Compared with Shipboard CTD on 11/07/2017')\n",
    "fig.subplots_adjust(top=0.9)\n",
    "\n",
    "legend = ax2.legend(loc='lower right', shadow=True, fontsize='small');\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As expected, the comparisons between profiler data from October 17 and Cuise CTD casting data from November 7 are not good.  Again, we don't have data from the profiler for the last ~20 days of its deployment, so this was more of an exercise.  I didn't find a more appropriately timed CTD cast for comparison.  "
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Notable Results and Actions Needed\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Based on this (somewhat cursory) analysis of the Pioneer Central Inshore Profiler (CP02PMCI), we note the following takeaways:\n",
    "* The profiler CTD recorded a set of consecutive outlier values during a single profile on 8/10/2017\n",
    "* Telemetered data from deployment 8 stops on 10/17/2017, short of the recorded deployment end date of 11/07/2017.\n",
    "* In general, besides the anomalous profile noted above, the temperature, salinity and pressure values during deployment 8 look largely reasonable.\n",
    "* The data seems to align with known physical processes, though additional datasets would need to be consulted to confirm.  This includes:\n",
    "  * A well defined two-layer system.\n",
    "  * The surface layer warms and deepens significantly through the summer\n",
    "  * There are several freshwater surface impulses during the late spring and early fall, which might confirm with rain events\n",
    "  * The pesistent warm/salty surfaces that persist in the fall may be related to an offshore Gulf Stream filament\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
