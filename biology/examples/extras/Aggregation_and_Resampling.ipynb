{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Aggregation and Resampling.ipynb",
   "version": "0.3.2",
   "views": {},
   "default_view": {},
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "OOI",
   "language": "python",
   "name": "ooi"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Aggregation and Resampling\n",
    "*Written by Friedrich Knuth, Rutgers University, May 21, 2018, as revised by Sage Lichtenwalner June 13, 2018*\n",
    "\n",
    "In this example we will learn how to programmatically download and work with OOI NetCDF data from within a notebook. \n",
    "\n",
    "We will use data from the 3D Thermistor Array deployed in the ASHES Vent field at Axial Seamount for this example, but the mechanics apply to all datasets that are processed through the OOI Cyberinfrastructure (CI) system. \n",
    "\n",
    "In this example, you will learn:\n",
    "* how to find the data you are looking for\n",
    "* how to use the machine to machine API to request data\n",
    "* how to load the NetCDF data into your notebook, once the data request has completed\n",
    "* how to explore and plot data\n",
    "\n",
    "A great resource for data wrangling and exploration in python can be found at https://chrisalbon.com/. Tip: add \"albon\" to your search in google when trying to find a common technique and chances are Chris Albon has made a post on how to do it.\n",
    "\n",
    "The difference between a NetCDF and JSON data request is that NetCDF files are served asynchronously and delivered to a THREDDS server, while the JSON data response is synchronous (instantaneous) and served as a JSON object in the GET response. NetCDF data is undecimated (full data set), while the JSON response is decimated down to a maximum of 20,000 data points.\n",
    "\n",
    "*Note, given the size of the dataset, parts of this notebook are rather processing intensive and it may not run well on Google Colab or your local machine.  High-performance computing solutions (e.g. [pangeo](http://pangeo-data.org)) might be a better option."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup your API Information\n",
    "Login in at https://ooinet.oceanobservatories.org/ and obtain your <b>API username and API token</b> under your profile (top right corner), or use the credentials provided below."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "username = ''\n",
    "token = ''"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Find and request the data"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import time"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The ingredients being used to build the data_request_url can be found here. For this example, we will use the data from the 3D Thermistor Array (TMPSF)\n",
    "http://ooi.visualocean.net/instruments/view/RS03ASHS-MJ03B-07-TMPSFA301\n",
    "\n",
    "![RS03ASHS-MJ03B-07-TMPSFA301](https://github.com/ooi-data-review/ooi_datateam_notebooks/raw/master/images/RS03ASHS-MJ03B-07-TMPSFA301.png)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "subsite = 'RS03ASHS'\n",
    "node = 'MJ03B'\n",
    "sensor = '07-TMPSFA301'\n",
    "method = 'streamed'\n",
    "stream = 'tmpsf_sample'\n",
    "beginDT = '2014-09-27T01:01:01.000Z' #begin of first deployement\n",
    "endDT = None"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "base_url = 'https://ooinet.oceanobservatories.org/api/m2m/12576/sensor/inv/'\n",
    "\n",
    "data_request_url ='/'.join((base_url,subsite,node,sensor,method,stream))\n",
    "params = {\n",
    "    'beginDT':beginDT,\n",
    "    'endDT':endDT,   \n",
    "}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Send the data request."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# r = requests.get(data_request_url, params=params, auth=(username, token))\n",
    "# data = r.json()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The first url in the response is the location on THREDDS where the data is being served. We will get back to using the THREDDS location later."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print(data['allURLs'][0])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The second url in the response is the regular APACHE server location for the data."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print(data['allURLs'][1])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We will use this second location to programmatically check for a status.txt file to be written, containing the text 'request completed'. This indicates that the request is completed and the system has finished writing out the data to this location. This step may take a few minutes."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%time\n",
    "check_complete = data['allURLs'][1] + '/status.txt'\n",
    "for i in range(1800): \n",
    "    r = requests.get(check_complete)\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        print('request completed')\n",
    "        break\n",
    "    else:\n",
    "        time.sleep(1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load the dataset into the notebook"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Copy the thredds url (from `print(data['allURLs'][0])`) and add it here so we can use it again later without having to regnerate the dataset."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "url = 'https://opendap.oceanobservatories.org/thredds/catalog/ooi/ooidatateam@gmail.com/20180517T182049-RS03ASHS-MJ03B-07-TMPSFA301-streamed-tmpsf_sample/catalog.html'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import re\n",
    "!pip install netcdf4\n",
    "!pip install dask\n",
    "!pip install xarray\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Paste the thredds url you received by downloading the data from the netcdf_data_requests notebook, or use the one provided below.  \n",
    "\n",
    "We will parse the html at the location where the files are being delivered to get the list of the NetCDF files written to THREDDS. Note that separate NetCDF files are created at 500 mb intervals and when there is a new deployment."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tds_url = 'https://opendap.oceanobservatories.org/thredds/dodsC'\n",
    "datasets = requests.get(url).text\n",
    "urls = re.findall(r'href=[\\'\"]?([^\\'\" >]+)', datasets)\n",
    "x = re.findall(r'(ooi/.*?.nc)', datasets)\n",
    "for i in x:\n",
    "    if i.endswith('.nc') == False:\n",
    "        x.remove(i)\n",
    "for i in x:\n",
    "    try:\n",
    "        float(i[-4])\n",
    "    except:\n",
    "        x.remove(i)\n",
    "datasets = [os.path.join(tds_url, i) for i in x]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "datasets"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Use xarray to open all netcdf files as a single xarray dataset, swap the dimension from obs to time and and examine the content."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Note this may take a while\n",
    "ds = xr.open_mfdataset(datasets)\n",
    "ds = ds.swap_dims({'obs': 'time'})\n",
    "ds = ds.chunk({'time': 100})\n",
    "ds"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Explore the dataset"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as dates\n",
    "import numpy as np"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Use built in xarray plotting functions to create simple line plot."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Given the amount of data, this may take a while to plot\n",
    "ds['temperature12'].plot()\n",
    "plt.show();"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can tell that the peak temperature is increasing, but this simple line plot does not reveal the internal data distribution. Let's convert to pandas dataframe and downsample from 1 Hz to 1/60 Hz. This step may take 5-10 minutes. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%time\n",
    "from dask.diagnostics import ProgressBar\n",
    "with ProgressBar():\n",
    "    df = ds['temperature12'].to_dataframe()\n",
    "    df = df.resample('min').mean()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%time\n",
    "plt.close()\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(16, 6)\n",
    "df['temperature12'].plot(ax=ax)\n",
    "df['temperature12'].resample('H').mean().plot(ax=ax)\n",
    "df['temperature12'].resample('D').mean().plot(ax=ax)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we are getting a better sense of the data. Let's convert time to ordinal, grab temperature values and re-examine using hexagonal bi-variate binning. Again, this step may take a few minutes."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%time\n",
    "time = []\n",
    "time_pd = pd.to_datetime(ds.time.values.tolist())\n",
    "for i in time_pd:\n",
    "    i = np.datetime64(i).astype(datetime.datetime)\n",
    "    time.append(dates.date2num(i)) "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "temperature = ds['temperature12'].values.tolist()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.close()\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(16, 6)\n",
    "\n",
    "hb1 = ax.hexbin(time, temperature, bins='log', vmin=0.4, vmax=3, gridsize=(1100, 100), mincnt=1, cmap='Blues')\n",
    "fig.colorbar(hb1)\n",
    "ax.yaxis.grid(True)\n",
    "ax.xaxis.grid(True)\n",
    "# ax.set_xlim(datetime.datetime(2015, 12, 1, 0, 0),datetime.datetime(2016, 7, 25, 0, 0))\n",
    "# ax.set_ylim(2,11)\n",
    "years = dates.YearLocator()\n",
    "months = dates.MonthLocator()\n",
    "yearsFmt = dates.DateFormatter('\\n\\n\\n%Y')\n",
    "monthsFmt = dates.DateFormatter('%b')\n",
    "ax.xaxis.set_major_locator(months)\n",
    "ax.xaxis.set_major_formatter(monthsFmt)\n",
    "ax.xaxis.set_minor_locator(years)\n",
    "ax.xaxis.set_minor_formatter(yearsFmt)\n",
    "plt.tight_layout()\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=90)\n",
    "plt.ylabel('Temperature $^\\circ$C')\n",
    "plt.xlabel('Time')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
