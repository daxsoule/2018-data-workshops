{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "OOI_Bio_Workshop_MJK.ipynb",
   "version": "0.3.2",
   "views": {},
   "default_view": {},
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python2",
   "display_name": "Python 2"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#**Data Validation for Argentine Basin Global Array Dissolved Oxygen Data**\n",
    "#OOI Biology Workshop Report\n",
    "\n",
    "Author: Matthew J Kupchik\n",
    "\n",
    "Date: 06/22/2018\n",
    "\n",
    "#0. Review Summary\n",
    "##*Dissolved Oxygen Measurement Comparisons Across Multiple Spatially and Temporally Similar Sensors*\n",
    "\n",
    "The original idea was developed by looking at the available data products, including the primary plots provided in the OOI data portal and OOI visualocean.net as to the availability of certain data streams and their coverage. This was done to determine user access, potential availability of methodology, and to determine probable workflows for end level users. A derived data product was selected due to the possibility of confounding errors with a multi-stage computational product.\n",
    "\n",
    "The first plan of action (POA) was determined to be analyzing the dissolved oxygen (DO) measuremnts for the Arentine Basin Global Array for a number of sensors as the Apex Surface Mooring and the Apex Profiler Mooring were closely related in space. Overlap with glider based profiling DO measuremnts made the ability to compare various sensors, entire water column, temporal relationships an obvious opportunity for identification of outliers in the data. This also happened to be some of the freshest Global Array data availble through the OOI data portal available for analysis. Initial results determined there were significant issues with the identified  first POA 2017 data available for deployment 3 across multiple sensors.\n",
    "\n",
    "The second POA was devised using the same Global Array, the Argentine Basin, but along the second deployment which spanned the majority of 2016. Upon analysis of the available data through the OOI data portal, a number of data issues became available during the end user workflow. Secondary analysis of available data sets showed some issues with data availability, inclduing sensor errors during deployment, which ultimately led to the production of variable quality for several DO data products. The results of this primary investigation into the data validation and completness for the Argentine Basin Global Array are summarized in the following report.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#1. Site, Sensors, and Time-Period Included in This Report\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##1.1 Platforms - Sensors"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "### 1.1.1 Apex Surface Mooring - GA01SUMO\n",
    "\n",
    "As the data summarized in the initial investigation are shown with associated data plots in the OOI data portal, it appears as if there is significant coverage for a number of the DO sensors throughout the 3rd deployment, which encompasses the end of 2016 and the majority of 2017. Gaps for a number of sensors in 2016 are evident, including a number of identified gaps for the fixed-depth, energized cable DO sensors. In all cases the sensor identified for the collection of DO data was the Optode 4831, manufactured by Aanderaa. [Mfg. LINK](https://www.aanderaa.com/productsdetail.php?Oxygen-Optodes-2)\n",
    "\n",
    "\n",
    "*   *Near Surface*  \n",
    "      * Dissolved Oxygen (GA01SUMO-RID16-06-DOSTAD000)\n",
    "          * ![alt text](https://i.imgur.com/nxuWre5.png)\n",
    "          * Timeframe = year 2016\n",
    "       \n",
    "*  *Mooring RIser*  \n",
    "      *   Dissolved Oxygen (40m) (GA01SUMO-RII11-02-DOSTAD031)\n",
    "          * ![alt text](https://imgur.com/nxnzbkl.png)\n",
    "          * Timeframe = year 2016\n",
    "          \n",
    "      *   Dissolved Oxygen (80 m) (GA01SUMO-RII11-02-DOSTAD032)\n",
    "          * ![alt text](https://imgur.com/Q5kC7L4.png)\n",
    "          * Timeframe = year 2016\n",
    "          \n",
    "      *   Dissolved Oxygen (130 m) (GA01SUMO-RII11-02-DOSTAD033)\n",
    "          * ![alt text](https://imgur.com/m7utKeR.png)\n",
    "          * Timeframe = year 2016\n",
    "    \n",
    "          \n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.1.2 Apex Profiler Mooring               - GA02HYPM\n",
    "*   *Wire-Following Profiler Upper*  \n",
    "      *   Dissolved Oxygen (GA02HYPM-WFP02-03-DOSTAL000)\n",
    "          * ![alt text](https://imgur.com/GZD0JaC.png)\n",
    "          * Timeframe = year 2016\n",
    "        \n",
    "*   *Wire-Following Profiler Lower*  \n",
    "      *    Dissolved Oxygen (GA02HYPM-WFP03-03-DOSTAL000)\n",
    "           * ![alt text](https://imgur.com/cN7BTvD.png)\n",
    "           * Timeframe = year 2016\n",
    "           \n",
    "     "
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "     \n",
    "### 1.1.3 Mobile Glider Assets               - GA05MOAS\n",
    "*   *Near Surface*  \n",
    "      *   Dissolved Oxygen (130 m) (GA01SUMO-RII11-02-DOSTAD033)\n",
    "          * ![alt text](https://imgur.com/RsMTnPS.png)\n",
    "          * Timeframe = year 2016\n",
    "          \n",
    "\n",
    "##1.2 Site Selected\n",
    "![alt text](http://oceanobservatories.org/wp-content/uploads/2015/09/CEV-OOI-Global-Argentine-Basin.jpg)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#2. Related Metadata - Check the metadata included in the report is correct\n",
    "\n",
    "The first goal is to determine if the metadata matches the instrument that is determined through both the OOI data portal, as well as through the oceanvisualization data team developed site.\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##2.1 First import the necesary libraries and the username and token information\n",
    "\n",
    "This step will bring in the necessary username and token information that will be required for the later informational data requests through the API, as well as install some basic libraries that will be useulf during the analysis."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "# Input username and token for the API\n",
    "USERNAME = 'OOIAPI-HKQ7NYEKTQWVNN'\n",
    "TOKEN =  'WZIQPE42M00GOC'\n",
    "\n",
    "# Install into the VM the necessary Python libraries\n",
    "!pip install pandas\n",
    "!pip install xarray\n",
    "!pip install netCDF4\n",
    "!pip install xarray\n",
    "!pip install cmocean\n",
    "\n",
    "# Import the necessary libraries into the virtual worksplace\n",
    "import requests\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import xarray as ar\n",
    "import matplotlib.pyplot  as plt\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "import cmocean"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##2.2 List the Sites of Importance Identified for the Metadata and Cross-Analysis\n",
    "\n",
    "Based on the summary of sites, moorings, nodes, and sensros provided previously, those sites need to be delineated as variables for later API calls for each of the various datsets needed to complete the cross comparison.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###2.2.1 Apex Surface Mooring\n",
    "\n",
    "The Apex surface mooring has a total of four (4) sensors to be included in the analysis. The first sensor is the \"surface\" DO sensor located at a depth of approximately 7 m depth. The other DO sensors are at fixed depths of 40 m , 80 m, and 130 m. The Surface sensor is located on a separate node from the other three sensors. All other sensors are located on the nergized cable below the buoy and contained within the same node. Each instrument has its own identifier, and all have the same method of data recovery for the isntruments due to a lack of telemetry data. The surface mooring also has a separate stream of data than the three deeper instruments."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "#Information for constructing the request \n",
    "# Site Information for the Surface Mooring\n",
    "Ssite = 'GA01SUMO'\n",
    "#Node Information for the surface mooring and the lower fixed DO sensors\n",
    "Snode1 = 'RID16'\n",
    "Snode2 = 'RII11'\n",
    "#Present the isntrument requests\n",
    "SinstrumentS = '06-DOSTAD000'\n",
    "Sinstrument40 = '02-DOSTAD031'\n",
    "Sinstrument80 = '02-DOSTAD032'\n",
    "Sinstrument130 = '02-DOSTAD033'\n",
    "#Method is the same for all of these instruments due to bad telemetry data for the timeframe\n",
    "method = 'recovered_host'\n",
    "#Stream data for each of the sintruments\n",
    "SstreamS = 'dosta_abcdjm_dcl_instrument_recovered'\n",
    "SstreamD = 'dosta_abcdjm_ctdbp_p_dcl_instrument_recovered'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###2.2.2 Apex Profiler Mooring\n",
    "The Apex Profiler Mooring has two dissolved oxygen sensors which are located as wire profiler packages. The upper wire profiler ranges from approximately 2200 meters to approximately 300 m depth. The lower wire profiler ranges from approximately 2300 m to a bottom depth of near 5000 m, the deepest part of the Argentine basin along the abyssal plain. Interestingly, the only difference in designation for these two sensors is the 'node' designation. All other data is consistent between the instruments. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "#Information for constructing the request \n",
    "# Site Information for the Surface Mooring\n",
    "Psite = 'GA02HYPM'\n",
    "#Node Information for the surface mooring and the lower fixed DO sensors\n",
    "Pnode1 = 'WFP02'\n",
    "Pnode2 = 'WFP03'\n",
    "#Present the isntrument requests - same instrument designator\n",
    "Pinstrument = '03-DOSTAL000'\n",
    "#Method is the same for all of these instruments due to bad telemetry data for the timeframe\n",
    "Pmethod = 'recovered_wfp'\n",
    "#Stream data for each of the sintruments\n",
    "Pstream = 'dosta_ln_wfp_instrument_recovered'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.2.3 Profiling Glider\n",
    "\n",
    "The profiling gliders are only part of the glider deployment for the Argentine Basin Global Array. Each of the gliders is part of the mobile asset designation. However, each is represented under a separate node todesignate the individual glider. As the gliders are seaparted into both profiling (near surface) and oceanic (deep-diving), the gliders need to be indidivually investigated to determine which vehicles correpsond individually to the data resources required for the analysis."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "#Information for constructing the request \n",
    "# Site Information for the Surface Mooring\n",
    "Gsite = 'GA05MOAS'\n",
    "#Node Information for the surface mooring and the lower fixed DO sensors\n",
    "Gnode = 'PG563'\n",
    "#Present the isntrument requests - same instrument designator\n",
    "Ginstrument = '02-DOSTAM000'\n",
    "#Method is the same for all of these instruments due to bad telemetry data for the timeframe\n",
    "Gmethod = 'recovered_host'\n",
    "#Stream data for each of the sintruments\n",
    "Gstream = 'dosta_abcdjm_glider_recovered'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##2.3 Pull Some of the Vocab Metadata Information\n",
    "\n",
    "The vocab metadata information will make sure we have identified the right instruments in our initial instrument call above and that the results are going to be provided by the predetermined and identified isntruments.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.3.1 Vocabulary Metadata for the Apex Surface Mooring\n",
    "\n",
    "Here we are going to set-up and determine that the identified sites, nodes, and instruments are the appropriate ones previously identified for the surface sensor, and the fixed depth sensors at 40 m, 80 m, and 130 m."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "#List the Vocab API\n",
    "VOCAB_API = 'https://ooinet.oceanobservatories.org/api/m2m/12586/vocab/inv'\n",
    "\n",
    "# Specify some functions to convert timestamps\n",
    "ntp_epoch = datetime.datetime(1900, 1, 1)\n",
    "unix_epoch = datetime.datetime(1970, 1, 1)\n",
    "ntp_delta = (unix_epoch - ntp_epoch).total_seconds()\n",
    "\n",
    "def ntp_seconds_to_datetime(ntp_seconds):\n",
    "    return datetime.datetime.utcfromtimestamp(ntp_seconds - ntp_delta).replace(microsecond=0)\n",
    "  \n",
    "def convert_time(ms):\n",
    "  if ms != None:\n",
    "    return datetime.datetime.utcfromtimestamp(ms/1000)\n",
    "  else:\n",
    "    return None\n",
    "\n",
    "# CHeck the SUrface Mooring Instrument\n",
    "# Setup the API request url\n",
    "vocabdata_request_urlS ='/'.join((VOCAB_API,Ssite,Snode1,SinstrumentS))\n",
    "print vocabdata_request_urlS\n",
    "# Grab the information from the server\n",
    "r = requests.get(vocabdata_request_urlS, auth=(USERNAME, TOKEN))\n",
    "vocabS = r.json()\n",
    "print vocabS\n",
    "\n",
    "# CHeck the 40 m Mooring Instrument\n",
    "# Setup the API request url\n",
    "vocabdata_request_url40 ='/'.join((VOCAB_API,Ssite,Snode2,Sinstrument40))\n",
    "print vocabdata_request_url40\n",
    "# Grab the information from the server\n",
    "r = requests.get(vocabdata_request_url40, auth=(USERNAME, TOKEN))\n",
    "vocab40 = r.json()\n",
    "print vocab40\n",
    "\n",
    "# CHeck the 80 m Mooring Instrument\n",
    "# Setup the API request url\n",
    "vocabdata_request_url80 ='/'.join((VOCAB_API,Ssite,Snode2,Sinstrument80))\n",
    "print vocabdata_request_url80\n",
    "# Grab the information from the server\n",
    "r = requests.get(vocabdata_request_url80, auth=(USERNAME, TOKEN))\n",
    "vocab80 = r.json()\n",
    "print vocab80\n",
    "\n",
    "# CHeck the 130 m Mooring Instrument\n",
    "# Setup the API request url\n",
    "vocabdata_request_url130 ='/'.join((VOCAB_API,Ssite,Snode2,Sinstrument130))\n",
    "print vocabdata_request_url130\n",
    "# Grab the information from the server\n",
    "r = requests.get(vocabdata_request_url130, auth=(USERNAME, TOKEN))\n",
    "vocab130 = r.json()\n",
    "print vocab130\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.3.2 Vocabulary Metadata for the Apex Profiler Mooring\n",
    "\n",
    "Setup and call the vocabulary metadat information for the upper water column and lower water column wire profilers to determine if the sensors are correct and that the apprropriate instrument is being referenced."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "# Check the Upper Profiler\n",
    "# Setup the API request url\n",
    "vocabdata_request_urlUP ='/'.join((VOCAB_API,Psite,Pnode1,Pinstrument))\n",
    "print vocabdata_request_urlUP\n",
    "# Grab the information from the server\n",
    "r = requests.get(vocabdata_request_urlUP, auth=(USERNAME, TOKEN))\n",
    "vocabUP = r.json()\n",
    "print vocabUP\n",
    "\n",
    "# Check the Lower Profiler\n",
    "# Setup the API request url\n",
    "vocabdata_request_urlLP ='/'.join((VOCAB_API,Psite,Pnode2,Pinstrument))\n",
    "print vocabdata_request_urlLP\n",
    "# Grab the information from the server\n",
    "r = requests.get(vocabdata_request_urlLP, auth=(USERNAME, TOKEN))\n",
    "vocabLP = r.json()\n",
    "print vocabLP\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.3.3 Vocabulary Metadata for Profiling Glider \n",
    "\n",
    "The appropriate data for the glider identified for the appropriate time period and the data associated to determine that the right glider and sensor are being referenced."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "# Check the Glider\n",
    "# Setup the API request url\n",
    "vocabdata_request_urlG ='/'.join((VOCAB_API,Gsite,Gnode,Ginstrument))\n",
    "print vocabdata_request_urlG\n",
    "# Grab the information from the server\n",
    "r = requests.get(vocabdata_request_urlG, auth=(USERNAME, TOKEN))\n",
    "vocabG = r.json()\n",
    "print vocabG"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##2.4 Deployment Data for the Inclusive Total Date Range of All Instruments\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###2.4.1 Apex Surface Mooring\n",
    "\n",
    "Setting the dates for the year 2016 for the Apex Surface Mooring to determine that the correct deployment is being requested for the year inclusive of 2016. This includes the surface mooring DO sensor at a depth of 7 m and the fixed depth sensors at depths of 40 m, 80 m, and 130 m.\n",
    "\n",
    "*Potential tightening of the code could be conducted with further time allotted to call the data within a loop to select only the nodes and instruments colected within a secondary nested structure.*"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "#Surface Mooring\n",
    "# Setup the API request url# Setup \n",
    "ASSET_API = 'https://ooinet.oceanobservatories.org/api/m2m/12587'\n",
    "data_request_urlSM = ASSET_API + '/events/deployment/query'\n",
    "params = {\n",
    "  'beginDT':'2016-01-01T00:00:00.000Z',\n",
    "  'endDT':'2017-01-01T00:00:00.000Z',\n",
    "  'refdes':Ssite+'-'+Snode1+'-'+SinstrumentS,   \n",
    "}\n",
    "# Grab the information from the server\n",
    "r = requests.get(data_request_urlSM, params=params, auth=(USERNAME, TOKEN))\n",
    "SM = r.json()\n",
    "\n",
    "#40m mooring\n",
    "# Setup the API request url# Setup \n",
    "ASSET_API = 'https://ooinet.oceanobservatories.org/api/m2m/12587'\n",
    "data_request_url40 = ASSET_API + '/events/deployment/query'\n",
    "params = {\n",
    "  'beginDT':'2016-01-01T00:00:00.000Z',\n",
    "  'endDT':'2017-01-01T00:00:00.000Z',\n",
    "  'refdes':Ssite+'-'+Snode2+'-'+Sinstrument40,   \n",
    "}\n",
    "# Grab the information from the server\n",
    "r = requests.get(data_request_url40, params=params, auth=(USERNAME, TOKEN))\n",
    "S40 = r.json()\n",
    "\n",
    "#80m mooring\n",
    "# Setup the API request url# Setup \n",
    "ASSET_API = 'https://ooinet.oceanobservatories.org/api/m2m/12587'\n",
    "data_request_url80 = ASSET_API + '/events/deployment/query'\n",
    "params = {\n",
    "  'beginDT':'2016-01-01T00:00:00.000Z',\n",
    "  'endDT':'2017-01-01T00:00:00.000Z',\n",
    "  'refdes':Ssite+'-'+Snode2+'-'+Sinstrument80,   \n",
    "}\n",
    "# Grab the information from the server\n",
    "r = requests.get(data_request_url80, params=params, auth=(USERNAME, TOKEN))\n",
    "S80 = r.json()\n",
    "\n",
    "#130m mooring\n",
    "# Setup the API request url# Setup \n",
    "ASSET_API = 'https://ooinet.oceanobservatories.org/api/m2m/12587'\n",
    "data_request_url130 = ASSET_API + '/events/deployment/query'\n",
    "params = {\n",
    "  'beginDT':'2016-01-01T00:00:00.000Z',\n",
    "  'endDT':'2017-01-01T00:00:00.000Z',\n",
    "  'refdes':Ssite+'-'+Snode2+'-'+Sinstrument130,   \n",
    "}\n",
    "# Grab the information from the server\n",
    "r = requests.get(data_request_url130, params=params, auth=(USERNAME, TOKEN))\n",
    "S130 = r.json()\n",
    "\n",
    "dSM = pd.DataFrame() # Setup empty array\n",
    "for d in SM:\n",
    "  dSM = dSM.append({\n",
    "      'deployment': d['deploymentNumber'],\n",
    "      'start': convert_time(d['eventStartTime']),\n",
    "      'stop': convert_time(d['eventStopTime']),\n",
    "      'latitude': d['location']['latitude'],\n",
    "      'longitude': d['location']['longitude'],\n",
    "      'sensor': d['sensor']['uid'],\n",
    "      'asset_id': d['sensor']['assetId'],\n",
    "    }, ignore_index=True)\n",
    "for d in S40:\n",
    "  dSM = dSM.append({\n",
    "      'deployment': d['deploymentNumber'],\n",
    "      'start': convert_time(d['eventStartTime']),\n",
    "      'stop': convert_time(d['eventStopTime']),\n",
    "      'latitude': d['location']['latitude'],\n",
    "      'longitude': d['location']['longitude'],\n",
    "      'sensor': d['sensor']['uid'],\n",
    "      'asset_id': d['sensor']['assetId'],\n",
    "    }, ignore_index=True)\n",
    "for d in S80:\n",
    "  dSM = dSM.append({\n",
    "      'deployment': d['deploymentNumber'],\n",
    "      'start': convert_time(d['eventStartTime']),\n",
    "      'stop': convert_time(d['eventStopTime']),\n",
    "      'latitude': d['location']['latitude'],\n",
    "      'longitude': d['location']['longitude'],\n",
    "      'sensor': d['sensor']['uid'],\n",
    "      'asset_id': d['sensor']['assetId'],\n",
    "    }, ignore_index=True)\n",
    "for d in S130:\n",
    "  dSM = dSM.append({\n",
    "      'deployment': d['deploymentNumber'],\n",
    "      'start': convert_time(d['eventStartTime']),\n",
    "      'stop': convert_time(d['eventStopTime']),\n",
    "      'latitude': d['location']['latitude'],\n",
    "      'longitude': d['location']['longitude'],\n",
    "      'sensor': d['sensor']['uid'],\n",
    "      'asset_id': d['sensor']['assetId'],\n",
    "    }, ignore_index=True)\n",
    "dSM"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###2.4.2 Apex Profiler Mooring\n",
    "\n",
    "Deployment information for the year 2016 for the two wire profiling packages to measure DO."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "#Upper Crawler\n",
    "# Setup the API request url# Setup \n",
    "ASSET_API = 'https://ooinet.oceanobservatories.org/api/m2m/12587'\n",
    "data_request_urlUC = ASSET_API + '/events/deployment/query'\n",
    "params = {\n",
    "  'beginDT':'2016-01-01T00:00:00.000Z',\n",
    "  'endDT':'2017-01-01T00:00:00.000Z',\n",
    "  'refdes':Psite+'-'+Pnode1+'-'+Pinstrument,   \n",
    "}\n",
    "# Grab the information from the server\n",
    "r = requests.get(data_request_urlUC, params=params, auth=(USERNAME, TOKEN))\n",
    "UC = r.json()\n",
    "\n",
    "#Lower Crawler\n",
    "# Setup the API request url# Setup \n",
    "ASSET_API = 'https://ooinet.oceanobservatories.org/api/m2m/12587'\n",
    "data_request_urlLC = ASSET_API + '/events/deployment/query'\n",
    "params = {\n",
    "  'beginDT':'2016-01-01T00:00:00.000Z',\n",
    "  'endDT':'2017-01-01T00:00:00.000Z',\n",
    "  'refdes':Psite+'-'+Pnode2+'-'+Pinstrument,   \n",
    "}\n",
    "# Grab the information from the server\n",
    "r = requests.get(data_request_urlLC, params=params, auth=(USERNAME, TOKEN))\n",
    "LC = r.json()\n",
    " \n",
    "\n",
    "dPF = pd.DataFrame() # Setup empty array\n",
    "for d in UC:\n",
    "  dPF = dPF.append({\n",
    "      'deployment': d['deploymentNumber'],\n",
    "      'start': convert_time(d['eventStartTime']),\n",
    "      'stop': convert_time(d['eventStopTime']),\n",
    "      'latitude': d['location']['latitude'],\n",
    "      'longitude': d['location']['longitude'],\n",
    "      'sensor': d['sensor']['uid'],\n",
    "      'asset_id': d['sensor']['assetId'],\n",
    "    }, ignore_index=True)\n",
    "for d in LC:\n",
    "  dPF = dPF.append({\n",
    "      'deployment': d['deploymentNumber'],\n",
    "      'start': convert_time(d['eventStartTime']),\n",
    "      'stop': convert_time(d['eventStopTime']),\n",
    "      'latitude': d['location']['latitude'],\n",
    "      'longitude': d['location']['longitude'],\n",
    "      'sensor': d['sensor']['uid'],\n",
    "      'asset_id': d['sensor']['assetId'],\n",
    "    }, ignore_index=True)\n",
    "dPF"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.4.3 Profiling Glider \n",
    "\n",
    "Deployment information for the profiling glider that matches up in space and time for the Argentine Basin Global Array for the deployment in 2016, in part."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "#Lower Crawler\n",
    "# Setup the API request url# Setup \n",
    "ASSET_API = 'https://ooinet.oceanobservatories.org/api/m2m/12587'\n",
    "data_request_urlPG = ASSET_API + '/events/deployment/query'\n",
    "params = {\n",
    "  'beginDT':'2016-01-01T00:00:00.000Z',\n",
    "  'endDT':'2017-01-01T00:00:00.000Z',\n",
    "  'refdes':Gsite+'-'+Gnode+'-'+Ginstrument,   \n",
    "}\n",
    "# Grab the information from the server\n",
    "r = requests.get(data_request_urlPG, params=params, auth=(USERNAME, TOKEN))\n",
    "PG = r.json()\n",
    "\n",
    "dPG = pd.DataFrame() # Setup empty array\n",
    "for d in PG:\n",
    "  dPG = dPG.append({\n",
    "      'deployment': d['deploymentNumber'],\n",
    "      'start': convert_time(d['eventStartTime']),\n",
    "      'stop': convert_time(d['eventStopTime']),\n",
    "      'latitude': d['location']['latitude'],\n",
    "      'longitude': d['location']['longitude'],\n",
    "      'sensor': d['sensor']['uid'],\n",
    "      'asset_id': d['sensor']['assetId'],\n",
    "    }, ignore_index=True)\n",
    "dPG"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##2.5 Calibration Information\n",
    "\n",
    "Calibration information is needed to determine that calibrations have been condcuted for each sensor for each deployment. Due to the extended nature of each Global Array deployment, calibration may result in drift from beginning to end. Although ther eare calibrations provided for each instrument, extended deployments may require further calibration profiles for extended environemtnal impacts on weak-link sensor components.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###2.5.1 Apex Surface Mooring\n",
    "\n",
    "The calls for the calibration information to pull the information for each of the the sensors (i.e., the 7m, 40m, 80m, and 130 m depth) included in the Apex Surface Mooring and the energized cable."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "# Setup the API request url - SM\n",
    "data_request_urlSM = ASSET_API + '/asset/cal'\n",
    "params = {\n",
    "  'beginDT':'2016-02-01T00:00:00.000Z',\n",
    "  'endDT':'2016-02-02T00:00:00.000Z',\n",
    "  'refdes':Ssite+'-'+Snode1+'-'+'06-DOSTAD000',\n",
    "}\n",
    "# Grab the information from the server\n",
    "r = requests.get(data_request_urlSM, params=params, auth=(USERNAME, TOKEN))\n",
    "CalSM1 = r.json()\n",
    "\n",
    "# Setup the API request url - 40 m\n",
    "data_request_url40 = ASSET_API + '/asset/cal'\n",
    "params = {\n",
    "  'beginDT':'2016-02-01T00:00:00.000Z',\n",
    "  'endDT':'2016-02-02T00:00:00.000Z',\n",
    "  'refdes':Ssite+'-'+Snode2+'-'+'02-DOSTAD031',\n",
    "}\n",
    "# Grab the information from the server\n",
    "r = requests.get(data_request_url40, params=params, auth=(USERNAME, TOKEN))\n",
    "Cal40 = r.json()\n",
    "\n",
    "# Setup the API request url - 80 m\n",
    "data_request_url80 = ASSET_API + '/asset/cal'\n",
    "params = {\n",
    "  'beginDT':'2016-02-01T00:00:00.000Z',\n",
    "  'endDT':'2016-02-02T00:00:00.000Z',\n",
    "  'refdes':Ssite+'-'+Snode2+'-'+'02-DOSTAD032',\n",
    "}\n",
    "# Grab the information from the server\n",
    "r = requests.get(data_request_url80, params=params, auth=(USERNAME, TOKEN))\n",
    "Cal80 = r.json()\n",
    "\n",
    "# Setup the API request url - 130 m\n",
    "data_request_url130 = ASSET_API + '/asset/cal'\n",
    "params = {\n",
    "  'beginDT':'2016-02-01T00:00:00.000Z',\n",
    "  'endDT':'2016-02-02T00:00:00.000Z',\n",
    "  'refdes':Ssite+'-'+Snode2+'-'+'02-DOSTAD033',\n",
    "}\n",
    "# Grab the information from the server\n",
    "r = requests.get(data_request_url130, params=params, auth=(USERNAME, TOKEN))\n",
    "Cal130 = r.json()\n",
    "\n",
    "# Reformat the data into a pretty table\n",
    "CalSM = pd.DataFrame() # Setup empty array\n",
    "for d in CalSM1:\n",
    "  for dd in d['sensor']['calibration']:\n",
    "    for ddd in dd['calData']:\n",
    "      CalSM = CalSM.append({\n",
    "        'value': ddd['value'],\n",
    "        'start': convert_time(ddd['eventStartTime']),\n",
    "        'stop': convert_time(ddd['eventStopTime']),\n",
    "        'name': ddd['eventName'],\n",
    "        'assetUid': ddd['assetUid'],\n",
    "        }, ignore_index=True)\n",
    "for d in Cal40:\n",
    "  for dd in d['sensor']['calibration']:\n",
    "    for ddd in dd['calData']:\n",
    "      CalSM = CalSM.append({\n",
    "        'value': ddd['value'],\n",
    "        'start': convert_time(ddd['eventStartTime']),\n",
    "        'stop': convert_time(ddd['eventStopTime']),\n",
    "        'name': ddd['eventName'],\n",
    "        'assetUid': ddd['assetUid'],\n",
    "        }, ignore_index=True)   \n",
    "for d in Cal80:\n",
    "  for dd in d['sensor']['calibration']:\n",
    "    for ddd in dd['calData']:\n",
    "      CalSM = CalSM.append({\n",
    "        'value': ddd['value'],\n",
    "        'start': convert_time(ddd['eventStartTime']),\n",
    "        'stop': convert_time(ddd['eventStopTime']),\n",
    "        'name': ddd['eventName'],\n",
    "        'assetUid': ddd['assetUid'],\n",
    "        }, ignore_index=True)\n",
    "for d in Cal130:\n",
    "  for dd in d['sensor']['calibration']:\n",
    "    for ddd in dd['calData']:\n",
    "      CalSM = CalSM.append({\n",
    "        'value': ddd['value'],\n",
    "        'start': convert_time(ddd['eventStartTime']),\n",
    "        'stop': convert_time(ddd['eventStopTime']),\n",
    "        'name': ddd['eventName'],\n",
    "        'assetUid': ddd['assetUid'],\n",
    "        }, ignore_index=True)\n",
    "      \n",
    "      \n",
    "CalSM = CalSM.sort_values(by=['start','name'])\n",
    "CalSM"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###2.5.2 Apex **Profiling** Mooring\n",
    "\n",
    "A pull for the calibration information that i relevant to the deployment of both the upper wire crawler and the lower wire crawler profilers from the Apex Profiler Mooring."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "ASSET_API = 'https://ooinet.oceanobservatories.org/api/m2m/12587'\n",
    "# Setup the API request url - UC\n",
    "data_request_urlUC = ASSET_API + '/asset/cal'\n",
    "params = {\n",
    "  'beginDT':'2016-01-15T00:00:00.000Z',\n",
    "  'endDT':'2016-01-16T00:00:00.000Z',\n",
    "  'refdes':Psite+'-'+Pnode1+'-'+'03-DOSTAL000',\n",
    "}\n",
    "# Grab the information from the server\n",
    "r = requests.get(data_request_urlUC, params=params, auth=(USERNAME, TOKEN))\n",
    "CalUC1 = r.json()\n",
    "\n",
    "# Setup the API request url - LC\n",
    "data_request_urlLC = ASSET_API + '/asset/cal'\n",
    "params = {\n",
    "  'beginDT':'2016-01-15T00:00:00.000Z',\n",
    "  'endDT':'2016-01-16T00:00:00.000Z',\n",
    "  'refdes':Psite+'-'+Pnode2+'-'+'03-DOSTAL000',\n",
    "}\n",
    "# Grab the information from the server\n",
    "r = requests.get(data_request_urlLC, params=params, auth=(USERNAME, TOKEN))\n",
    "CalLC1 = r.json()\n",
    "\n",
    "# Reformat the data into a pretty table\n",
    "CalPM = pd.DataFrame() # Setup empty array\n",
    "for d in CalUC1:\n",
    "  for dd in d['sensor']['calibration']:\n",
    "    for ddd in dd['calData']:\n",
    "      CalPM = CalPM.append({\n",
    "        'value': ddd['value'],\n",
    "        'start': convert_time(ddd['eventStartTime']),\n",
    "        'stop': convert_time(ddd['eventStopTime']),\n",
    "        'name': ddd['eventName'],\n",
    "        'assetUid': ddd['assetUid'],\n",
    "        }, ignore_index=True)\n",
    "    \n",
    "\n",
    "CalPM = CalPM.sort_values(by=['start','name'])\n",
    "CalPM"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###2.5.3 Profiling Glider\n",
    "\n",
    "A call for the profiling glider information for the deployment necessary for the comparison."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "# Setup the API request url - Glider\n",
    "data_request_urlG = ASSET_API + '/asset/cal'\n",
    "params = {\n",
    "  'beginDT':'2016-02-01T00:00:00.000Z',\n",
    "  'endDT':'2016-02-02T00:00:00.000Z',\n",
    "  'refdes':Gsite+'-'+Gnode+'-'+'02-DOSTAM000',\n",
    "}\n",
    "\n",
    "# Grab the information from the server\n",
    "r = requests.get(data_request_urlG, params=params, auth=(USERNAME, TOKEN))\n",
    "CalPG1 = r.json()\n",
    "\n",
    "# Reformat the data into a pretty table\n",
    "CalPG = pd.DataFrame() # Setup empty array\n",
    "for d in CalPG1:\n",
    "  for dd in d['sensor']['calibration']:\n",
    "    for ddd in dd['calData']:\n",
    "      CalPG = CalPG.append({\n",
    "        'value': ddd['value'],\n",
    "        'start': convert_time(ddd['eventStartTime']),\n",
    "        'stop': convert_time(ddd['eventStopTime']),\n",
    "        'name': ddd['eventName'],\n",
    "        'assetUid': ddd['assetUid'],\n",
    "        }, ignore_index=True)\n",
    "    \n",
    "CalPG = CalPG.sort_values(by=['start','name'])\n",
    "CalPG"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.6 Annotations for the Instruments\n",
    "\n",
    "Annotation information that has already been entered into the OOI data portal to help end-level users with understanding known issues, gaps, and potential interpretability issues with the associated data structure.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###2.6.1 Annotations for Surface Mooring\n",
    "\n",
    "The call for the annotations for the four sensors associated with the surface mooring and the known issues that have already been described for the 2016 data."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "#For the SUrface Mooring\n",
    "ANNO_API_SM = 'https://ooinet.oceanobservatories.org/api/m2m/12580/anno/find'\n",
    "params = {\n",
    "  'beginDT':int(datetime.date(2016,1,1).strftime('%s'))*1000,\n",
    "  'endDT':int(datetime.date(2017,1,1).strftime('%s'))*1000,\n",
    "  'refdes':Ssite+'-'+Snode1+'-'+SinstrumentS,\n",
    "}\n",
    "#Make the request for data\n",
    "r = requests.get(ANNO_API_SM, params=params, auth=(USERNAME, TOKEN))\n",
    "AnSM1 = r.json()\n",
    "\n",
    "#For the 40\n",
    "ANNO_API_40 = 'https://ooinet.oceanobservatories.org/api/m2m/12580/anno/find'\n",
    "params = {\n",
    "  'beginDT':int(datetime.date(2016,1,1).strftime('%s'))*1000,\n",
    "  'endDT':int(datetime.date(2017,1,1).strftime('%s'))*1000,\n",
    "  'refdes':Ssite+'-'+Snode2+'-'+Sinstrument40,\n",
    "}\n",
    "#Make the request for data\n",
    "r = requests.get(ANNO_API_40, params=params, auth=(USERNAME, TOKEN))\n",
    "An40 = r.json()\n",
    "\n",
    "#For the 80\n",
    "ANNO_API_80 = 'https://ooinet.oceanobservatories.org/api/m2m/12580/anno/find'\n",
    "params = {\n",
    "  'beginDT':int(datetime.date(2016,1,1).strftime('%s'))*1000,\n",
    "  'endDT':int(datetime.date(2017,1,1).strftime('%s'))*1000,\n",
    "  'refdes':Ssite+'-'+Snode2+'-'+Sinstrument80,\n",
    "}\n",
    "#Make the request for data\n",
    "r = requests.get(ANNO_API_80, params=params, auth=(USERNAME, TOKEN))\n",
    "An80 = r.json()\n",
    "\n",
    "#For the 130\n",
    "ANNO_API_130 = 'https://ooinet.oceanobservatories.org/api/m2m/12580/anno/find'\n",
    "params = {\n",
    "  'beginDT':int(datetime.date(2016,1,1).strftime('%s'))*1000,\n",
    "  'endDT':int(datetime.date(2017,1,1).strftime('%s'))*1000,\n",
    "  'refdes':Ssite+'-'+Snode2+'-'+Sinstrument130,\n",
    "}\n",
    "#Make the request for data\n",
    "r = requests.get(ANNO_API_130, params=params, auth=(USERNAME, TOKEN))\n",
    "An130 = r.json()\n",
    "\n",
    "AnSM = pd.DataFrame() # Setup empty array\n",
    "for d in AnSM1:\n",
    "  AnSM = AnSM.append({\n",
    "    'annotation': d['annotation'],\n",
    "    'start': convert_time(d['beginDT']),\n",
    "    'stop': convert_time(d['endDT']),\n",
    "    'site': d['subsite'],\n",
    "    'node': d['node'],\n",
    "    'sensor': d['sensor'],\n",
    "    'id': d['id']\n",
    "  }, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', -1) # Show the full annotation text\n",
    "for d in An40:\n",
    "  AnSM = AnSM.append({\n",
    "    'annotation': d['annotation'],\n",
    "    'start': convert_time(d['beginDT']),\n",
    "    'stop': convert_time(d['endDT']),\n",
    "    'site': d['subsite'],\n",
    "    'node': d['node'],\n",
    "    'sensor': d['sensor'],\n",
    "    'id': d['id']\n",
    "  }, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', -1) # Show the full annotation text\n",
    "for d in An80:\n",
    "  AnSM = AnSM.append({\n",
    "    'annotation': d['annotation'],\n",
    "    'start': convert_time(d['beginDT']),\n",
    "    'stop': convert_time(d['endDT']),\n",
    "    'site': d['subsite'],\n",
    "    'node': d['node'],\n",
    "    'sensor': d['sensor'],\n",
    "    'id': d['id']\n",
    "  }, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', -1) # Show the full annotation text\n",
    "for d in An130:\n",
    "  AnSM = AnSM.append({\n",
    "    'annotation': d['annotation'],\n",
    "    'start': convert_time(d['beginDT']),\n",
    "    'stop': convert_time(d['endDT']),\n",
    "    'site': d['subsite'],\n",
    "    'node': d['node'],\n",
    "    'sensor': d['sensor'],\n",
    "    'id': d['id']\n",
    "  }, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', -1) # Show the full annotation text\n",
    "AnSM"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###2.6.2 Annotations for Profiling Mooring\n",
    "\n",
    "The call for the annotations to pul known issues with data associated with each of the two wire profilers associated with the profiling mooring in response to the DO sensor. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "#For the UC\n",
    "ANNO_API_UC = 'https://ooinet.oceanobservatories.org/api/m2m/12580/anno/find'\n",
    "params = {\n",
    "  'beginDT':int(datetime.date(2016,1,1).strftime('%s'))*1000,\n",
    "  'endDT':int(datetime.date(2017,1,1).strftime('%s'))*1000,\n",
    "  'refdes':Psite+'-'+Pnode1+'-'+Pinstrument,\n",
    "}\n",
    "#Make the request for data\n",
    "r = requests.get(ANNO_API_UC, params=params, auth=(USERNAME, TOKEN))\n",
    "AnUC = r.json()\n",
    "\n",
    "#For the LC\n",
    "ANNO_API_LC = 'https://ooinet.oceanobservatories.org/api/m2m/12580/anno/find'\n",
    "params = {\n",
    "  'beginDT':int(datetime.date(2016,1,1).strftime('%s'))*1000,\n",
    "  'endDT':int(datetime.date(2017,1,1).strftime('%s'))*1000,\n",
    "  'refdes':Psite+'-'+Pnode2+'-'+Pinstrument,\n",
    "}\n",
    "#Make the request for data\n",
    "r = requests.get(ANNO_API_LC, params=params, auth=(USERNAME, TOKEN))\n",
    "AnLC = r.json()\n",
    "\n",
    "\n",
    "AnPM = pd.DataFrame() # Setup empty array\n",
    "for d in AnUC:\n",
    "  AnPM = AnPM.append({\n",
    "    'annotation': d['annotation'],\n",
    "    'start': convert_time(d['beginDT']),\n",
    "    'stop': convert_time(d['endDT']),\n",
    "    'site': d['subsite'],\n",
    "    'node': d['node'],\n",
    "    'sensor': d['sensor'],\n",
    "    'id': d['id']\n",
    "  }, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', -1) # Show the full annotation text\n",
    "for d in AnLC:\n",
    "  AnPM = AnPM.append({\n",
    "    'annotation': d['annotation'],\n",
    "    'start': convert_time(d['beginDT']),\n",
    "    'stop': convert_time(d['endDT']),\n",
    "    'site': d['subsite'],\n",
    "    'node': d['node'],\n",
    "    'sensor': d['sensor'],\n",
    "    'id': d['id']\n",
    "  }, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', -1) # Show the full annotation text\n",
    "AnPM"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###2.6.3 Annotations for Profiling Glider\n",
    "\n",
    "An API pull of the known data issues with  profiling glider data structure. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "ANNO_API_PG = 'https://ooinet.oceanobservatories.org/api/m2m/12580/anno/find'\n",
    "params = {\n",
    "  'beginDT':int(datetime.date(2017,1,1).strftime('%s'))*1000,\n",
    "  'endDT':int(datetime.date(2018,1,1).strftime('%s'))*1000,\n",
    "  'refdes':Gsite+'-'+Gnode+'-'+Ginstrument,\n",
    "}\n",
    "\n",
    "r = requests.get(ANNO_API_PG, params=params, auth=(USERNAME, TOKEN))\n",
    "AnPG1 = r.json()\n",
    "\n",
    "AnPG = pd.DataFrame() # Setup empty array\n",
    "for d in AnPG1:\n",
    "  AnPG = AnPG.append({\n",
    "    'annotation': d['annotation'],\n",
    "    'start': convert_time(d['beginDT']),\n",
    "    'stop': convert_time(d['endDT']),\n",
    "    'site': d['subsite'],\n",
    "    'node': d['node'],\n",
    "    'sensor': d['sensor'],\n",
    "    'id': d['id']\n",
    "  }, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', -1) # Show the full annotation text\n",
    "AnPG"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. The Full Datasets for Cross Analyses\n",
    "\n",
    "Datasets needed to be requested asynchronously from the OOI data server. The follwoing code blocks are desinged to make the requests for the necessary data to be supplied for further analysis of the DO data. The following code has been commented out as it has already been requested and served by the THREDDS data server.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.1 The Dissolved Oxygen Data for the Surface Mooring\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.1.1 Information for the Data Call of the 7 m Depth Sensor"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "#API base url\n",
    "#SENSOR_BASE_URL = 'https://ooinet.oceanobservatories.org/api/m2m/12576/sensor/inv/'\n",
    "\n",
    "# Create the full request URL\n",
    "#data_request_urlSM ='/'.join((SENSOR_BASE_URL,Ssite,Snode1,SinstrumentS,method,SstreamS))\n",
    "#params = {\n",
    "#  'beginDT':'2016-01-01T00:00:00.000Z',\n",
    "#  'endDT':'2017-01-01T00:00:00.000Z',\n",
    "#  'format':'application/netcdf',\n",
    "#  'include_provenance':'true',\n",
    "#  'include_annotations':'true'\n",
    "#}\n",
    "#\n",
    "#r = requests.get(data_request_urlSM, params=params, auth=(USERNAME, TOKEN))\n",
    "#DataSM = r.json()\n",
    "\n",
    "#DataSM\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.1.2 Information for the Data Call of the 40 m Depth Sensor"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "#Request the 40m data\n",
    "#data_request_url40 ='/'.join((SENSOR_BASE_URL,Ssite,Snode2,Sinstrument40,method,SstreamD))\n",
    "\n",
    "#params = {\n",
    "#  'beginDT':'2016-01-01T00:00:00.000Z',\n",
    "#  'endDT':'2017-01-01T00:00:00.000Z',\n",
    "#  'format':'application/netcdf',\n",
    "#  'include_provenance':'true',\n",
    "#  'include_annotations':'true'\n",
    "#}\n",
    "\n",
    "#r = requests.get(data_request_url40, params=params, auth=(USERNAME, TOKEN))\n",
    "#Data40 = r.json()\n",
    "\n",
    "#Data40"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.1.3 Information for the Data Call of the 80 m Depth Sensor"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "#Request the 80m data\n",
    "#data_request_url80 ='/'.join((SENSOR_BASE_URL,Ssite,Snode2,Sinstrument80,method,SstreamD))\n",
    "\n",
    "#params = {\n",
    "#  'beginDT':'2016-01-01T00:00:00.000Z',\n",
    "#  'endDT':'2017-01-01T00:00:00.000Z',\n",
    "#  'format':'application/netcdf',\n",
    "#  'include_provenance':'true',\n",
    "#  'include_annotations':'true'\n",
    "#}\n",
    "\n",
    "#r = requests.get(data_request_url80, params=params, auth=(USERNAME, TOKEN))\n",
    "#Data80 = r.json()\n",
    "\n",
    "#Data80"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.1.4 Information for the Data Call of the 130 m Depth Sensor"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "#data_request_url130 ='/'.join((SENSOR_BASE_URL,Ssite,Snode2,Sinstrument130,method,SstreamD))\n",
    "\n",
    "#params = {\n",
    "#  'beginDT':'2016-01-01T00:00:00.000Z',\n",
    "#  'endDT':'2017-01-01T00:00:00.000Z',\n",
    "#  'format':'application/netcdf',\n",
    "#  'include_provenance':'true',\n",
    "#  'include_annotations':'true'\n",
    "#}\n",
    "\n",
    "#r = requests.get(data_request_url130, params=params, auth=(USERNAME, TOKEN))\n",
    "#Data130 = r.json()\n",
    "\n",
    "#Data130"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.2 The Dissolved Oxygen Data for the Profiling Mooring\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.2.1 Information for the Data Call of the Upper Wire Crawler"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "# API base url\n",
    "#SENSOR_BASE_URL = 'https://ooinet.oceanobservatories.org/api/m2m/12576/sensor/inv'\n",
    "\n",
    "#data_request_urlUC1 ='/'.join((SENSOR_BASE_URL,'GA02HYPM','WFP02','03-DOSTAL000','recovered_wfp','dosta_ln_wfp_instrument_recovered'))\n",
    "\n",
    "#params = {\n",
    "#  'beginDT':'2016-01-01T00:00:00.000Z',\n",
    "#  'endDT':'2017-01-01T00:00:00.000Z',\n",
    "#  'format':'application/netcdf',\n",
    "#  'include_provenance':'true',\n",
    "#  'include_annotations':'true'\n",
    "#}\n",
    "\n",
    "#r = requests.get(data_request_urlUC1, params=params, auth=(USERNAME, TOKEN))\n",
    "#DataUC = r.json()\n",
    "\n",
    "#DataUC\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.2.2 Information for the Data Call of the Lower Wire Crawler"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "#data_request_urlLC ='/'.join((SENSOR_BASE_URL,'GA02HYPM','WFP03','03-DOSTAL000','recovered_wfp','dosta_ln_wfp_instrument_recovered'))\n",
    "\n",
    "#params = {\n",
    "#  'beginDT':'2016-01-01T00:00:00.000Z',\n",
    "#  'endDT':'2017-01-01T23:00:00.000Z',\n",
    "#  'format':'application/netcdf',\n",
    "#  'include_provenance':'true',\n",
    "#  'include_annotations':'true'\n",
    "#}\n",
    "\n",
    "#r = requests.get(data_request_urlLC, params=params, auth=(USERNAME, TOKEN))\n",
    "#DataLC = r.json()\n",
    "\n",
    "#DataLC"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.3 The Dissolved Oxygen Data for the Profiling Glider\n",
    "\n",
    "### 3.3.1 Information for the Data Call of the Glider"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "#data_request_urlPG ='/'.join((SENSOR_BASE_URL,Gsite,Gnode,Ginstrument,Gmethod,Gstream))\n",
    "\n",
    "#params = {\n",
    "#  'beginDT':'2016-01-01T00:00:00.000Z',\n",
    "#  'endDT':'2017-01-01T00:00:00.000Z',\n",
    "#  'format':'application/netcdf',\n",
    "#  'include_provenance':'true',\n",
    "#  'include_annotations':'true'\n",
    "#}\n",
    "\n",
    "#r = requests.get(data_request_urlPG, auth=(USERNAME, TOKEN))\n",
    "#DataPG = r.json()\n",
    "\n",
    "#DataPG"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#4. Opening the Data Files from Each of the Instruments\n",
    "\n",
    "After confirmation that the OOI data server has provided the necessary NETcdf files, the follwing blocks of code read in and allocate the necessary data sets for plotting and analysis issues.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##4.1 Bringing in the Data From the Surface Mooring\n",
    "\n",
    "It becomes apparent from the access of data into an xarray data structure that the available data from the surface DO sensor (~7m) and those at the other fixed depths (40m, 80m, 130m) are in different formats, have differnet calculations, and result in different variables. This creates issues for end level users without the access to information on how data structures are appropriated for the various streams."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "!pip install netCDF4\n",
    "import netCDF4 as nc\n",
    "\n",
    "!pip install xarray\n",
    "import xarray as xr\n",
    "\n",
    "!pip install cmocean\n",
    "import cmocean\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "data_url_SM01 = 'https://opendap.oceanobservatories.org/thredds/dodsC/ooi/kupzgod@gmail.com/20180621T150333-GA01SUMO-RID16-06-DOSTAD000-recovered_host-dosta_abcdjm_dcl_instrument_recovered/deployment0002_GA01SUMO-RID16-06-DOSTAD000-recovered_host-dosta_abcdjm_dcl_instrument_recovered_20160101T000008.371000-20161108T091800.474000.nc'\n",
    "# Open the dataset\n",
    "SM01 = xr.open_dataset(data_url_SM01)\n",
    "\n",
    "# Swap the dimensions\n",
    "SM01 = SM01.swap_dims({'obs': 'time'})"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "data_url_SM40 = 'https://opendap.oceanobservatories.org/thredds/dodsC/ooi/kupzgod@gmail.com/20180621T150356-GA01SUMO-RII11-02-DOSTAD031-recovered_host-dosta_abcdjm_ctdbp_p_dcl_instrument_recovered/deployment0002_GA01SUMO-RII11-02-DOSTAD031-recovered_host-dosta_abcdjm_ctdbp_p_dcl_instrument_recovered_20160101T002321-20160428T172321.nc'\n",
    "\n",
    "# Open the dataset\n",
    "SM40 = xr.open_dataset(data_url_SM40)\n",
    "\n",
    "# Swap the dimensions\n",
    "SM40 = SM40.swap_dims({'obs': 'time'})\n",
    "\n",
    "SM40"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "data_url_SM80 = 'https://opendap.oceanobservatories.org/thredds/dodsC/ooi/kupzgod@gmail.com/20180621T150409-GA01SUMO-RII11-02-DOSTAD032-recovered_host-dosta_abcdjm_ctdbp_p_dcl_instrument_recovered/deployment0002_GA01SUMO-RII11-02-DOSTAD032-recovered_host-dosta_abcdjm_ctdbp_p_dcl_instrument_recovered_20160101T000003-20160604T040003.nc'\n",
    "\n",
    "# Open the dataset\n",
    "SM80 = xr.open_dataset(data_url_SM80)\n",
    "\n",
    "# Swap the dimensions\n",
    "SM80 = SM80.swap_dims({'obs': 'time'})"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "data_url_SM130 = 'https://opendap.oceanobservatories.org/thredds/dodsC/ooi/kupzgod@gmail.com/20180621T151525-GA01SUMO-RII11-02-DOSTAD033-recovered_host-dosta_abcdjm_ctdbp_p_dcl_instrument_recovered/deployment0002_GA01SUMO-RII11-02-DOSTAD033-recovered_host-dosta_abcdjm_ctdbp_p_dcl_instrument_recovered_20160101T000003-20160412T060003.nc'\n",
    "\n",
    "# Open the dataset\n",
    "SM130 = xr.open_dataset(data_url_SM130)\n",
    "\n",
    "# Swap the dimensions\n",
    "SM130 = SM130.swap_dims({'obs': 'time'})"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4.2 Bringing in the Data from the Profiling Mooring\n",
    "\n",
    "Accessing the profiling mooring data from the Argentine Basin Profiling Mooring's two wire profilers shows that the derived products are similar to the surface DO sensor, but radically different from those provided for the fixed depth sensors, despite all sensors being the same."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "data_url_PMUC = 'https://opendap.oceanobservatories.org/thredds/dodsC/ooi/kupzgod@gmail.com/20180621T150436-GA02HYPM-WFP02-03-DOSTAL000-recovered_wfp-dosta_ln_wfp_instrument_recovered/deployment0002_GA02HYPM-WFP02-03-DOSTAL000-recovered_wfp-dosta_ln_wfp_instrument_recovered_20160101T080202-20161104T160956.nc'\n",
    "\n",
    "# Open the dataset\n",
    "PMUC = xr.open_dataset(data_url_PMUC)\n",
    "\n",
    "# Swap the dimensions\n",
    "PMUC = PMUC.swap_dims({'obs': 'time'})\n",
    "PMUC"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "data_url_PMLC = 'https://opendap.oceanobservatories.org/thredds/dodsC/ooi/kupzgod@gmail.com/20180621T150515-GA02HYPM-WFP03-03-DOSTAL000-recovered_wfp-dosta_ln_wfp_instrument_recovered/deployment0002_GA02HYPM-WFP03-03-DOSTAL000-recovered_wfp-dosta_ln_wfp_instrument_recovered_20160101T120201-20161104T200957.nc'\n",
    "\n",
    "# Open the dataset\n",
    "PMLC = xr.open_dataset(data_url_PMLC)\n",
    "\n",
    "# Swap the dimensions\n",
    "PMLC = PMLC.swap_dims({'obs': 'time'})"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4.3 Bringing in the Data from the Profiling Glider\n",
    "\n",
    "The glider has yet a third structure of data products, completely separate in stream, designation ,and construction from the other sensors. This is despite the sensor beign the same on all platforms."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "data_url_PG = 'https://opendap.oceanobservatories.org/thredds/dodsC/ooi/kupzgod@gmail.com/20180620T205255-GA05MOAS-PG578-02-DOSTAM000-telemetered-dosta_abcdjm_glider_instrument/deployment0001_GA05MOAS-PG578-02-DOSTAM000-telemetered-dosta_abcdjm_glider_instrument_20161101T020536.654050-20170619T035413.048280.nc'\n",
    "# Open the dataset\n",
    "PG = xr.open_dataset(data_url_PG)\n",
    "\n",
    "# Swap the dimensions\n",
    "PG = PG.swap_dims({'obs': 'time'})\n",
    "\n",
    "#PG\n",
    "#x1 = plt.plot(PG['time'],PG['sci_oxy4_oxygen'],linestyle='None',marker='.',markersize=5,color='red')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 5. Plotting the Data From the Working Datasets\n",
    "\n",
    "Initial plotting of the data will help to show where, or if, issues in the data structure become available.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5.1 Plotting the D.O. Data from the Fixed Depth Sensors\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.1.1 Data Plotted for the Surface Sensor (7 m)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,9))\n",
    "\n",
    "ax1.plot(SM01['time'],SM01['dissolved_oxygen'],linestyle='None',marker='.',markersize=5,color='red')\n",
    "ax1.set_xlabel('Deployment 2 - 2017')\n",
    "ax1.set_ylabel('Dissolved Oxygen')\n",
    "\n",
    "ax2.hist(SM01['dissolved_oxygen']);"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.1.2 Data Plotted for the 40 m Depth Sensor"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,9))\n",
    "\n",
    "ax1.plot(SM40['time'],SM40['oxy_temp_volts'],linestyle='None',marker='.',markersize=5,color='red')\n",
    "ax1.plot(SM40['time'],SM40['oxy_calphase_volts'],linestyle='None',marker='.',markersize=5,color='blue')\n",
    "ax1.set_xlabel('Deployment 2 - 2017')\n",
    "ax1.set_ylabel('Dissolved Oxygen - Temp - Volts')\n",
    "\n",
    "ax2.hist(SM80['oxy_temp_volts']);"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.1.3 Data Plotted for the 80 m Depth Sensor"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,9))\n",
    "\n",
    "ax1.plot(SM80['time'],SM80['oxy_temp_volts'],linestyle='None',marker='.',markersize=5,color='red')\n",
    "ax1.plot(SM80['time'],SM80['oxy_calphase_volts'],linestyle='None',marker='.',markersize=5,color='blue')\n",
    "ax1.set_xlabel('Deployment 2 - 2017')\n",
    "ax1.set_ylabel('Dissolved Oxygen - Temp - Volts')\n",
    "\n",
    "ax2.hist(SM80['oxy_temp_volts']);"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.1.4 Data Plotted for the 130 m Depth Sensor"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,9))\n",
    "\n",
    "ax1.plot(SM130['time'],SM130['oxy_temp_volts'],linestyle='None',marker='.',markersize=5,color='red')\n",
    "ax1.plot(SM130['time'],SM130['oxy_calphase_volts'],linestyle='None',marker='.',markersize=5,color='blue')\n",
    "ax1.set_xlabel('Deployment 2 - 2017')\n",
    "ax1.set_ylabel('Dissolved Oxygen - Temp - Volts')\n",
    "\n",
    "ax2.hist(SM130['oxy_temp_volts']);"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5.2 Plotting the Data from the Profiling Sensors"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.2.1 Dissolved Oxygen Data from the Upper Wire Profiler"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,9))\n",
    "\n",
    "ax1.plot(PMUC['time'],PMUC['dissolved_oxygen'],linestyle='None',marker='.',markersize=5,color='red')\n",
    "ax1.set_xlabel('Deployment 2 - 2017')\n",
    "ax1.set_ylabel('Dissolved Oxygen')\n",
    "\n",
    "ax2.hist(PMUC['dissolved_oxygen']);"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.2.2 Dissolved Oxygen Data from the Lower Wire Profiler"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,9))\n",
    "\n",
    "ax1.plot(PMLC['time'],PMLC['dissolved_oxygen'],linestyle='None',marker='.',markersize=5,color='red')\n",
    "ax1.set_xlabel('Deployment 2 - 2017')\n",
    "ax1.set_ylabel('Dissolved Oxygen')\n",
    "\n",
    "ax2.hist(PMLC['dissolved_oxygen']);"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5.3 Plot the Two Wire Profilers Together to make sure that Features match up in Space & Time"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "dtime = PMUC['time'].values\n",
    "pressure = PMUC['int_ctd_pressure'].values\n",
    "do = PMUC['dissolved_oxygen'].values\n",
    "temp = PMUC['ctdpf_ckl_seawater_temperature'].values\n",
    "sal = PMUC['practical_salinity'].values\n",
    "\n",
    "\n",
    "dtime1 = PMLC['time'].values\n",
    "pressure1 = PMLC['int_ctd_pressure'].values\n",
    "do1 = PMLC['dissolved_oxygen'].values\n",
    "temp1 = PMLC['ctdpf_ckl_seawater_temperature'].values\n",
    "sal1 = PMLC['practical_salinity'].values"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.3.1 Dissolved Oxygen Values for the Two Profilers Together, and Individually"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(3,1, sharex=True, sharey=True, figsize=(16,8))\n",
    "sc1 = ax1.scatter(dtime, pressure, c=do, cmap=cmocean.cm.thermal, s=2, vmin=100, vmax=300,)\n",
    "cbar = fig.colorbar(sc1, ax=ax1, orientation='vertical', extend='both')\n",
    "sc2 = ax1.scatter(dtime1, pressure1, c=do1, cmap=cmocean.cm.thermal, s=2, vmin=100, vmax=300,)\n",
    "\n",
    "sc3 = ax2.scatter(dtime, pressure, c=do, cmap=cmocean.cm.thermal, s=2, vmin=100, vmax=300,)\n",
    "cbar = fig.colorbar(sc3, ax=ax2, orientation='vertical', extend='both')\n",
    "\n",
    "sc3 = ax3.scatter(dtime1, pressure1, c=do1, cmap=cmocean.cm.thermal, s=2, vmin=100, vmax=300,)\n",
    "cbar = fig.colorbar(sc3, ax=ax3, orientation='vertical', extend='both')\n",
    "\n",
    "# Because the X and Y axes are shared, we only have to set limits once\n",
    "ax1.invert_yaxis() # Invert y axis\n",
    "ax1.set_xlim(dtime[0],dtime[-1]) # Set the time limits to match the dataset\n",
    "\n",
    "ax1.set_ylabel('Pressure (db)')\n",
    "ax2.set_ylabel('Pressure (db)');\n",
    "ax3.set_ylabel('Pressure (db)');\n",
    "\n",
    "#fig.suptitle('CP02PMUO Deployment 9')\n",
    "#fig.subplots_adjust(top=0.95);"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.3.2 Salinity values for the Two Profilers Together, and Individually"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(3,1, sharex=True, sharey=True, figsize=(16,8))\n",
    "sc1 = ax1.scatter(dtime, pressure, c=sal, cmap=cmocean.cm.haline, s=2)\n",
    "sc2 = ax1.scatter(dtime1, pressure1, c=sal1, cmap=cmocean.cm.thermal, s=2)\n",
    "cbar = fig.colorbar(sc1, ax=ax1, orientation='vertical', extend='both')\n",
    "\n",
    "sc3 = ax2.scatter(dtime, pressure, c=sal, cmap=cmocean.cm.haline, s=2)\n",
    "cbar = fig.colorbar(sc3, ax=ax2, orientation='vertical', extend='both')\n",
    "\n",
    "sc3 = ax3.scatter(dtime1, pressure1, c=sal1, cmap=cmocean.cm.haline, s=2)\n",
    "cbar = fig.colorbar(sc3, ax=ax3, orientation='vertical', extend='both')\n",
    "\n",
    "# Because the X and Y axes are shared, we only have to set limits once\n",
    "ax1.invert_yaxis() # Invert y axis\n",
    "ax1.set_xlim(dtime[0],dtime[-1]) # Set the time limits to match the dataset\n",
    "\n",
    "ax1.set_ylabel('Pressure (db)')\n",
    "ax2.set_ylabel('Pressure (db)');\n",
    "ax3.set_ylabel('Pressure (db)');"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "BADPMUC = PMUC.where(PMUC['practical_salinity'] < 30)\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.3.3 Output of the Bad values identified from the salinity information used to develop the L2 DO Product"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "import matplotlib.pyplot as plt\n",
    "x1 = plt.plot(BADPMUC['time'],BADPMUC['dissolved_oxygen'],linestyle='None',marker='.',markersize=5,color='red')\n",
    "#plt.ylim(276,282);\n",
    "plt.xlabel('Date');\n",
    "plt.ylabel('Dissolved Oxygen')\n",
    "plt.xticks(rotation=90);"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5.4 Plot the Glider Data in 3 DImensions to Check for Missing Sections or Abnormalities in Glider Operations"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.scatter3D(PG['lat'], PG['lon'], PG['int_ctd_pressure']*-1, c=PG['sci_oxy4_oxygen'], cmap=cmocean.cm.thermal, vmin=200, vmax=300);\n",
    "\n",
    "ax.view_init(0, 35)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.scatter3D(PG['lat'], PG['lon'], PG['int_ctd_pressure']*-1, c=PG['sci_oxy4_oxygen'], cmap=cmocean.cm.thermal, vmin=200, vmax=300);\n",
    "\n",
    "ax.view_init(60, 35)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.scatter3D(PG['lat'], PG['lon'], PG['int_ctd_pressure']*-1, c=PG['sci_oxy4_oxygen'], cmap=cmocean.cm.thermal, vmin=200, vmax=300);\n",
    "\n",
    "ax.view_init(90, 35)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5.5 Deconstruct the Surface Time Series Data to Determine Data Value Issues"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "SM02 = SM01.to_dataframe();\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#@title\n",
    "from pylab import rcParams\n",
    "\n",
    "decompfreq = (24*60*60)/(2);\n",
    "\n",
    "decomposition = sm.tsa.seasonal_decompose(SM02['dissolved_oxygen'], freq=decompfreq, model='additive')\n",
    "fig = decomposition.plot()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 6. Notable Results and Actions Needed\n",
    "\n",
    "Based on this preliminary analysis from the perspective of an end-user, the Argentine Basin Global Array has some heavy issues with Dissolved Oxygen measurements. However, general trends and large scale features are still readily apparent in the data and could be used for basic feature identification for sensors or integration with bio-acoustic data, provided that data is without major issue. The following list includes some notable information encountered during the data validation workflow.\n",
    "\n",
    "\n",
    "*   During Data Identification\n",
    "    * A number of the datasets show strong data coverage for 2017 in Deployment 3, however have limited or zero actual data for dissolved oxygen L2 products\n",
    "    * Identification of data was difficut to dtermine the extent, coverage, and availability of the data within the OOI data portal.\n",
    "    * The OOI data portal showed graphs, plotted with data that either did not exist, or was not the correct variable noted for analysis.\n",
    "*   During I/O Operations\n",
    "    * Listed methods include dashes (i.e, wfp-recovered), which should be listed as underscores. This is likely a developer issue but needs to be solved for end level users to accurately access a number of the datasets.\n",
    "    * Data from the fixed depth DO sensors appear to have totally different variables and different calculation streams\n",
    "    * The end products have different variable names and missing or damaged calibration records based on Vendor reports\n",
    "* During Data Assessment\n",
    "    * For the surface DO sensor after detrending there were clearly several outliers in the data that would not be expected with either the seasonal or the daily variability\n",
    "    * There were clear features visible with the upper and lower water column profilers that suggest the datat to be well correlated between the isntruments\n",
    "    * Glider data shows that there are several times where the glider was not completing the proposed tracks, iehter sitting at the surface or being located further down in the water column.\n",
    "\n"
   ]
  }
 ]
}
