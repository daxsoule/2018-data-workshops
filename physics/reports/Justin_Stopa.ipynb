{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Justin Stopa OOI Data Quality Report.ipynb",
   "version": "0.3.2",
   "views": {},
   "default_view": {},
   "provenance": [
    {
     "file_id": "1qxEUp03PyvWZjA1ySrtWGSmjp1JBpNTN",
     "timestamp": 1527019538039
    },
    {
     "file_id": "1783o_5T7i9x9viaSGD-5hhSMmhimdOyG",
     "timestamp": 1523618562907
    }
   ],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python2",
   "display_name": "Python 2"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Global Southern Ocean Apex Surface Mooring (GS01SUMO) Data Quality Report\n",
    "**Evaluation Date**: 5/22/2018\n",
    "\n",
    "**Evaluator**: Justin Stopa\n",
    "\n",
    "## Review Summary\n",
    "\n",
    "Review Summary\n",
    "This report summarizes an example data quality review of the Global Southern Ocean Apex Surface Mooring (GS01SUMO).  In addition to the wave data, I also use wind speeds from the METBK sensors which measure wind speeds. My goal is to understand the data quality from recovered datasets. My particular interest is to know how many large events (Hs>10 m) are captured and the quality of those events. The eventual goal is to use these data as reference for calibration of different satellite technologies such as altimeters and synthetic aperture radars.\n",
    "\n",
    "This report follows the steps below\n",
    "\n",
    "1) Data Availability \n",
    "  *  What data are available? Are they relevant to the study you wish to pursue?\n",
    "\n",
    "2) Metadata \n",
    "  * What metadata are available? Are they complete? Is anything missing? What does it tell you about the dataset (for good or bad)?\n",
    "\n",
    "3) Understand the context\n",
    "  * Plot a large range of data. Does it look right based on what you would expect? What are the ranges, and do they make sense?\n",
    "\n",
    "4) Focus on one or more smaller ranges\n",
    "  * Plot some smaller periods (in space or time) to see if they look correct or have issues.\n",
    "\n",
    "5) Environmental Comparisons \n",
    "  * Compare the dataset with other datasets (e.g. from the cruise/shipboard)\n",
    "\n",
    "6) Summary and conclusion\n",
    "\n",
    "Due to the report location, I use data from a wave hindcast as well as wave data from altimeters to cross-validate the buoy observations. This analysis is not included in this ipynb but images are included in the cooresponding document. This particular location is interesting due to its remote location and high occurrence of large sea states. We summarize a few notes and recommendations at the end.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Selected instruments included in this report\n",
    "In this report, we will evaluate the surface wave buoy (specifically wave spectra) from the Global Southern Ocean Apex Surface Mooring (GS01SUMO), focusing on the summer of 2017. Additional instruments on the mooring are listed for reference and could be used for further evaluation.\n",
    "\n",
    "We focus on the recovered data stream in this review because they might be more data available.  There are 2 deployments I will focus on (1 & 2).  This can be seen here:\n",
    "http://ooi.visualocean.net/instruments/view/GS01SUMO-SBD12-05-WAVSSA000#deployments\n",
    "\n",
    "\n",
    "Deployment |\tCruise |\tStart Date | Stop Date |\tMooring Asset |\tNode Asset |\tSensor Asset |\tLatitude |\tLongitude |\tDeployment Depth |\tWater Depth\n",
    "  -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --\n",
    " 1 |\tAT-26-29 |\t02/18/2015 |\t12/27/2015 |\tCGMGS-01SUMO-00001 | \t|\tCGINS-WAVSSA-05971 |\t-54.4083 |\t-89.3575 |\t0 |\t4611\n",
    "2 | NBP-15-11 |\t12/14/2015 |\t12/12/2016 |\tCGMGS-01SUMO-00002 |\t|\tCGINS-WAVSSA-04571 |\t-54.4041 |\t-89.2069 |\t0 |\t4588\n",
    "3 |\tNBP16-10 |\t11/25/2016 |\t|\tCGMGS-01SUMO-00003 |\t|\tCGINS-WAVSSA-05971 |\t-54.4076 |\t-89.3567 |\t0 |\t4612\n",
    "\n",
    "\n",
    "Additionally a graph of data availability is available here:\n",
    "http://ooi.visualocean.net/instruments/view/GS01SUMO-SBD12-05-WAVSSA000\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Setup Instrument Variables\n",
    "site = 'GS01SUMO'\n",
    "node = 'SBD12'\n",
    "instrument = '05-WAVSSA000'\n",
    "method = 'recovered_host'\n",
    "stream = 'wavss_a_dcl_statistics_recovered'\n",
    "# Non-directional wave spectra\n",
    "streamS = 'wavss_a_dcl_non_directional_recovered'\n",
    "# Buoy displacement: x,y,z\n",
    "# wavss_a_dcl_motion_recovered\n",
    "# Wave (2D) frequency-direction wave spectra?  I am not completely sure... \n",
    "# wavss_a_dcl_mean_directional_recovered\n",
    "# Fourier... no idea what this is...\n",
    "# stream= 'wavss_a_dcl_fourier_recovered'\n",
    "\n",
    "# Met sensors (wind speed)\n",
    "instrumentA='06-METBKA000'\n",
    "methodA = 'recovered_host'\n",
    "# raw data wind u and v at anemometer height\n",
    "# stream = 'metbk_a_dcl_instrument_recovered'\n",
    "# flux measurements - has a U10 at neutral stability\n",
    "streamA ='metbk_hourly'\n",
    "# met_wind10m\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1b. Time periods of interest\n",
    "We will focus on the following time periods for evaluation:\n",
    "* Deployment 1: 02/18/2015 to 02/27/2015\n",
    "* Deployment 2: 12/14/2015 to 12/12/2016\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Related Metadata\n",
    "In this section, we will review some of metadata available in the system to make sure it is present and correct.\n",
    "\n",
    "Before we get started, we need to set up our Python environment with some libraries, variables and functions we will need later in this report."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Setup the Python processing environment \n",
    "import requests\n",
    "import datetime\n",
    "import pandas as pd\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# API Information\n",
    "USERNAME ='OOIAPI-WITS2KL3NKBNT4'\n",
    "TOKEN= 'TEMP-TOKEN-N36TKZ9B0FA5M3'\n",
    "DATA_API = 'https://ooinet.oceanobservatories.org/api/m2m/12576/sensor/inv'\n",
    "VOCAB_API = 'https://ooinet.oceanobservatories.org/api/m2m/12586/vocab/inv'\n",
    "ASSET_API = 'https://ooinet.oceanobservatories.org/api/m2m/12587'\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Specify some functions to convert timestamps\n",
    "ntp_epoch = datetime.datetime(1900, 1, 1)\n",
    "unix_epoch = datetime.datetime(1970, 1, 1)\n",
    "ntp_delta = (unix_epoch - ntp_epoch).total_seconds()\n",
    "\n",
    "def ntp_seconds_to_datetime(ntp_seconds):\n",
    "    return datetime.datetime.utcfromtimestamp(ntp_seconds - ntp_delta).replace(microsecond=0)\n",
    "  \n",
    "def convert_time(ms):\n",
    "  if ms != None:\n",
    "    return datetime.datetime.utcfromtimestamp(ms/1000)\n",
    "  else:\n",
    "    return None\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2a. Vocabulary Metadata\n",
    "First, I grab some basic vocabulary information (metadata) from the system to make sure we have the right instrument."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Setup the API request url\n",
    "data_request_url ='/'.join((VOCAB_API,site,node,instrument))\n",
    "print data_request_url\n",
    "# Grab the information from the server\n",
    "r = requests.get(data_request_url, auth=(USERNAME, TOKEN))\n",
    "data = r.json()\n",
    "print data\n",
    "\n",
    "# Do same for met data\n",
    "data_request_urlA ='/'.join((VOCAB_API,site,node,instrumentA))\n",
    "print data_request_urlA\n",
    "rA = requests.get(data_request_urlA, auth=(USERNAME, TOKEN))\n",
    "dataA = rA.json()\n",
    "print dataA"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "All this looks good!"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2b. Deployment Information\n",
    "Next, let's grab some information about the deployments for this instrument.  We will grab all of the recovered deployments available in the system and then output the date ranges, latitude/longitude, asset ID, and sensor ID for each.  Note that the **reference designator** specified above represents the geographical location of an instrument across all deployments (e.g. the wave buoy on the Global Southern Ocean site), the **Sensor ID** (and its Asset ID equivalent) represent the specific instrument used for a given deployment (i.e. a unique make, model, and serial numbered instrument)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Setup the API request url\n",
    "data_request_url = ASSET_API + '/events/deployment/query'\n",
    "params = {\n",
    "  'beginDT':'2015-02-18T00:00:00.000Z',\n",
    "  'endDT':'2016-12-12T00:00:00.000Z',\n",
    "  'refdes':site+'-'+node+'-'+instrument,   \n",
    "}\n",
    "\n",
    "# Grab the information from the server\n",
    "r = requests.get(data_request_url, params=params, auth=(USERNAME, TOKEN))\n",
    "data = r.json()\n",
    "\n",
    "df = pd.DataFrame() # Setup empty array\n",
    "for d in data:\n",
    "  df = df.append({\n",
    "      'deployment': d['deploymentNumber'],\n",
    "      'start': convert_time(d['eventStartTime']),\n",
    "      'stop': convert_time(d['eventStopTime']),\n",
    "      'latitude': d['location']['latitude'],\n",
    "      'longitude': d['location']['longitude'],\n",
    "      'sensor': d['sensor']['uid'],\n",
    "      'asset_id': d['sensor']['assetId'],\n",
    "    }, ignore_index=True)\n",
    "print df"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2c. Annotations\n",
    "Let's pull any relevant annotations for the instrument."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# block for wave statistics\n",
    "ANNO_API = 'https://ooinet.oceanobservatories.org/api/m2m/12580/anno/find'\n",
    "params = {\n",
    "  'beginDT':int(datetime.date(2015,12,1).strftime('%s'))*1000,\n",
    "  'endDT':int(datetime.date(2017,1,1).strftime('%s'))*1000,\n",
    "  'refdes':site+'-'+node+'-'+instrument,\n",
    "}\n",
    "r = requests.get(ANNO_API, params=params, auth=(USERNAME, TOKEN))\n",
    "data = r.json()\n",
    "df = pd.DataFrame() # Setup empty array\n",
    "for d in data:\n",
    "  df = df.append({\n",
    "    'annotation': d['annotation'],\n",
    "    'start': convert_time(d['beginDT']),\n",
    "    'stop': convert_time(d['endDT']),\n",
    "    'site': d['subsite'],\n",
    "    'node': d['node'],\n",
    "    'sensor': d['sensor'],\n",
    "    'id': d['id']\n",
    "  }, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', -1) # Show the full annotation text\n",
    "print df\n",
    "\n",
    "# Block for Met data\n",
    "ANNO_API = 'https://ooinet.oceanobservatories.org/api/m2m/12580/anno/find'\n",
    "params = {\n",
    "  'beginDT':int(datetime.date(2015,12,1).strftime('%s'))*1000,\n",
    "  'endDT':int(datetime.date(2017,1,1).strftime('%s'))*1000,\n",
    "  'refdes':site+'-'+node+'-'+instrumentA,\n",
    "}\n",
    "r = requests.get(ANNO_API, params=params, auth=(USERNAME, TOKEN))\n",
    "data = r.json()\n",
    "dfA = pd.DataFrame() # Setup empty array\n",
    "for d in data:\n",
    "  dfA = dfA.append({\n",
    "    'annotation': d['annotation'],\n",
    "    'start': convert_time(d['beginDT']),\n",
    "    'stop': convert_time(d['endDT']),\n",
    "    'site': d['subsite'],\n",
    "    'node': d['node'],\n",
    "    'sensor': d['sensor'],\n",
    "    'id': d['id']\n",
    "  }, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', -1) # Show the full annotation text\n",
    "print dfA"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "->For wave spectra: there is an annotations currently in the system, and it's for deployment 2: surface buoy was bent and the METBK-PRC sensor antenna was missing.\n",
    "\n",
    "-> For wind speeds there are 2 annotations currently in the system: \n",
    "deployment 2 wind speeds are biased low (5%) compared to ship measurements\n",
    "deployment 3:  At deployment (and continuing for several months), the sea surface temperature sensor exhibited random spikes. In the beginning of the deployment, the conductivity and salinity measured by the two met sensors were offset by ~0.01 S m-1 and ~0.1 psu, respectively (SBD12 was lower). The offset decreased over time, and gradually switched to SBD12 values being slightly higher (around Nov 2017). There was also a LWR (longwave radiation) offset of ~5 W m-2.   \n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Understand the context - the full dataset\n",
    "Now let's take a look at a large range of data; roughly 2 years of data (Feb 2015-Dec 2015).  \n",
    "Adding some  libraries to the Python environment."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "!pip install netCDF4\n",
    "import netCDF4 as nc\n",
    "\n",
    "!pip install xarray\n",
    "import xarray as xr\n",
    "\n",
    "!pip install cmocean\n",
    "import cmocean\n",
    "\n",
    "!pip install dask\n",
    "import dask\n",
    "\n",
    "\n",
    "import math\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For simplicity, I first request data from the \"Data Portal\" to make a download request for all available data.  After receiving the email, I looked through the results and I am finding all files (netcdf) associated with the deployment.  Using these data files, I will create a few different plots to better understand the available data."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "url='https://opendap.oceanobservatories.org/thredds/catalog/ooi/justin-stopa-gmail/20180522T205739-GS01SUMO-SBD12-05-WAVSSA000-recovered_host-wavss_a_dcl_statistics_recovered/catalog.html'\n",
    "tds_url = 'https://opendap.oceanobservatories.org/thredds/dodsC'\n",
    "datasets = requests.get(url).text\n",
    "urls = re.findall(r'href=[\\'\"]?([^\\'\" >]+)', datasets)\n",
    "x = re.findall(r'(ooi/.*?.nc)', datasets)\n",
    "for i in x:\n",
    "    if i.endswith('.nc') == False:\n",
    "        x.remove(i)\n",
    "for i in x:\n",
    "    try:\n",
    "        float(i[-4])\n",
    "    except:\n",
    "        x.remove(i)\n",
    "datasets = [os.path.join(tds_url, i) for i in x]\n",
    "print datasets\n",
    "\n",
    "dp = xr.open_mfdataset(datasets)\n",
    "dp = dp.swap_dims({'obs': 'time'})\n",
    "dp = dp.chunk({'time': 100})\n",
    "dp = dp.sortby('time') # data from different deployments can overlap so we want to sort all data by time stamp.\n",
    "print dp\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "url='https://opendap.oceanobservatories.org/thredds/catalog/ooi/justin-stopa-gmail/20180523T194039-GS01SUMO-SBD12-05-WAVSSA000-recovered_host-wavss_a_dcl_non_directional_recovered/catalog.html'\n",
    "tds_url = 'https://opendap.oceanobservatories.org/thredds/dodsC'\n",
    "datasetsS = requests.get(url).text\n",
    "urls = re.findall(r'href=[\\'\"]?([^\\'\" >]+)', datasetsS)\n",
    "x = re.findall(r'(ooi/.*?.nc)', datasetsS)\n",
    "for i in x:\n",
    "    if i.endswith('.nc') == False:\n",
    "        x.remove(i)\n",
    "for i in x:\n",
    "    try:\n",
    "        float(i[-4])\n",
    "    except:\n",
    "        x.remove(i)\n",
    "datasetsS = [os.path.join(tds_url, i) for i in x]\n",
    "\n",
    "\n",
    "ds = xr.open_mfdataset(datasetsS)\n",
    "ds = ds.swap_dims({'obs': 'time'})\n",
    "ds = ds.chunk({'time': 100})\n",
    "ds = ds.sortby('time') # data from different deployments can overlap so we want to sort all data by time stamp.\n",
    "print ds\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Block for met data\n",
    "url='https://opendap.oceanobservatories.org/thredds/catalog/ooi/justin-stopa-gmail/20180523T192152-GS01SUMO-SBD11-06-METBKA000-recovered_host-metbk_hourly/catalog.html'\n",
    "tds_url = 'https://opendap.oceanobservatories.org/thredds/dodsC'\n",
    "datasetsA = requests.get(url).text\n",
    "urls = re.findall(r'href=[\\'\"]?([^\\'\" >]+)', datasetsA)\n",
    "x = re.findall(r'(ooi/.*?.nc)', datasetsA)\n",
    "for i in x:\n",
    "    if i.endswith('.nc') == False:\n",
    "        x.remove(i)\n",
    "for i in x:\n",
    "    try:\n",
    "        float(i[-4])\n",
    "    except:\n",
    "        x.remove(i)\n",
    "datasetsA = [os.path.join(tds_url, i) for i in x]\n",
    "datasetsA\n",
    "\n",
    "# find how to properly index\n",
    "# From Sage start at index 0 and stride by 3\n",
    "#d=datasetsA[0::3]\n",
    "b=[datasetsA[x] for x in [0,3]]\n",
    "\n",
    "\n",
    "dA = xr.open_mfdataset(b)\n",
    "dA = dA.swap_dims({'obs': 'time'})\n",
    "dA = dA.chunk({'time': 100})\n",
    "dA = dA.sortby('time') # data from different deployments can overlap so we want to sort all data by time stamp.\n",
    "dA"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let's create a quick timeseries plot of significant wave height.  "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Original plot\n",
    "#ds['significant_wave_height'].plot()\n",
    "\n",
    "# Modifed plot\n",
    "# fig, ax = plt.subplots(figsize=(16,6))\n",
    "# sty='default'\n",
    "# mpl.style.use(sty)\n",
    "# ax.set_title('Wave Conditions SO Surface BUOY', color='C0')\n",
    "# ax.plot(dp['time'],dp['significant_wave_height'], 'C1', label='Hs (m)')\n",
    "# ax.plot(dp['time'],dp['peak_wave_period'], 'C2', label='Tp (s)')\n",
    "# ax.legend()\n",
    "\n",
    "plt.close()\n",
    "fig, axes = plt.subplots(5, sharex=True)\n",
    "fig.set_size_inches(16,9)\n",
    "\n",
    "p0 = axes[0].scatter(dp['time'].data,dp['significant_wave_height'].data,label='Hs (m)',color='k')\n",
    "axes[0].set_title('Hs (m)')\n",
    "\n",
    "p1 = axes[1].scatter(dp['time'].data,dp['peak_wave_period'].data,label='Tp (s)',color='b')\n",
    "axes[1].set_title('Tp (s)')\n",
    "\n",
    "p2 = axes[2].scatter(dp['time'].data,dp['mean_direction'].data,label='Dr (deg)',color='g')\n",
    "axes[2].set_title('Dr (deg)')\n",
    "\n",
    "p3 = axes[3].scatter(dp['time'].data,dp['number_zero_crossings'].data,label='N ()',color='m')\n",
    "axes[3].set_title('Number of waves crossing the zero ()')\n",
    "\n",
    "p4 = axes[4].scatter(dA['time'].data,dA['met_wind10m'].data,label='U10N (m/s)',color='r')\n",
    "axes[4].set_title('U10N (m/s)')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "At a first glance the data seems reasonable and consistent with the wave climate of the region. The sea state is very active with average seas above Hs>4 m. The wave heights are large due to the fact that there is a large and presumably infinite fetch available to generate waves (i.e. the large and unimpeded expanse of ocean available in the Southern Ocean).\n",
    "\n",
    "There is one stretch of missing data and only a few points that drop below the lower limit of Hs>2.5 m. \n",
    "\n",
    "Additional the peak period and average direction (dominantly from the WSW) seem reasonable.\n",
    "\n",
    "The U10N has a lot of missing data for this 1-year period.  Several values are less than 2 m/s which seem odd for this location and are mostly incorrect. \n",
    "\n",
    "Notice that the number of waves crossing zero changes between deployment 1 and deployment 2.  This is concerning because it suggests that the time the buoy measurements were taken changes.  The wave statistics computed using different averaging times could indeed be different. This was also seen in the wave spectra (see cooresponding pdf).\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#dp['significant_wave_height'].plot();\n",
    "\n",
    "# Now wind speed and Hs \n",
    "fig, ax = plt.subplots(figsize=(16,6))\n",
    "sty='default'\n",
    "mpl.style.use(sty)\n",
    "ax.set_title('Wind and wave conditions SO Surface BUOY', color='C0')\n",
    "ax.plot(dp['time'],dp['significant_wave_height'], 'C1', label='Hs (m)')\n",
    "ax.plot(dA['time'],dA['met_wind10m'], 'C2', label='U10N (m/s)')\n",
    "ax.legend()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To deepen the context. I included a plot from Stopa et al., 2013 OCEMOD showing the average wave height (Hs) and average wave direction from a wave hindcast using 30 years of data (1980-2009). This shows on average our location (lon=-89.20691W, lat=-54.40408) or approximately (270E,55S) typically has wave heights of at least 5 m throughout the year. Also notice that wave direction of WSW is consistent with the average wave direction plotted above. \n",
    "\n",
    "\n",
    "![Wave Climate- Seasonality from a wave hindcast (1980-2009)](https://drive.google.com/file/d/1Hlez7uILhujMvi_vbg176W_8fdb2RD12/view)\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here are some histograms to understand the range of data.  The histogram of Hs should follow a Rayleigh distribution. It should be relatively easy to pick out the suspected 'bad' values less than Hs of 2 m seen in the time series plots. It seems there are only 2 'bad' values (will confirm later).\n",
    "\n",
    "I made a similar plot the the wind speed (U10N) this should follow a Weibull distribution.  This should also be relatively easy to pick out outliers or wrong data values.  The annotation file states that the wind speeds were biased low by 5% for deployment 2 shown here."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(16,6))\n",
    "dp['significant_wave_height'].plot.hist(bins=100, ax=ax1)\n",
    "dA['met_wind10m'].plot.hist(bins=100, ax=ax2);\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "There are some obvious outliers in both datasets.  There only seems to be few bad points for Hs and several for U10N. I am concerned about the underestimation of the wind speeds as noted in the annotation.  I will try and confirm this with independent data from models and possibly satellite observations. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. A closer look\n",
    "Now we look at the day of maximum wave height (maximum of Hs).  In addition we look at December 2015 when the transition between deployment 1 and deployment 2 occured. \n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Find the time of the maximum Hs\n",
    "xid=dp.where(dp['significant_wave_height']==dp['significant_wave_height'].max(),drop=True)\n",
    "xid['time'].values\n",
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Look at data near the maximum wave height\n",
    "# it seems the Hs==0 is the incorrect\n",
    "plt.close()\n",
    "fig, axes = plt.subplots(4, sharex=True)\n",
    "fig.set_size_inches(16,9)\n",
    "\n",
    "p0 = axes[0].plot(dp['time'].data,dp['significant_wave_height'].data,label='Hs (m)',color='k')\n",
    "axes[0].set_title('Hs (m)')\n",
    "\n",
    "p1 = axes[1].plot(dp['time'].data,dp['peak_wave_period'].data,label='Tp (s)',color='b')\n",
    "axes[1].set_title('Tp (s)')\n",
    "\n",
    "p2 = axes[2].plot(dp['time'].data,dp['mean_direction'].data,label='Dr (deg)',color='g')\n",
    "axes[2].set_title('Dr (deg)')\n",
    "\n",
    "#p3 = axes[3].scatter(dA['time'].data,dA['met_wind10m'].data,label='U10N (m/s)',color='r')\n",
    "axes[3].set_title('U10N (m/s)')\n",
    "\n",
    "plt.xlim('2016-08-21T00:00:00','2016-08-22T00:00:00')\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Data seems reasonable except when Hs is zero. This point will be removed."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Now explore December 2015 when the transition between deployment 1 and 2 occurred\n",
    "plt.close()\n",
    "fig, axes = plt.subplots(4, sharex=True)\n",
    "fig.set_size_inches(12,6)\n",
    "\n",
    "p0 = axes[0].scatter(dp['time'].data,dp['significant_wave_height'].data,label='Hs (m)',color='k')\n",
    "axes[0].set_title('Hs (m)')\n",
    "\n",
    "p1 = axes[1].scatter(dp['time'].data,dp['peak_wave_period'].data,label='Tp (s)',color='b')\n",
    "axes[1].set_title('Tp (s)')\n",
    "\n",
    "p2 = axes[2].scatter(dp['time'].data,dp['mean_direction'].data,label='Dr (deg)',color='g')\n",
    "axes[2].set_title('Dr (deg)')\n",
    "\n",
    "p3 = axes[3].scatter(dp['time'].data,dp['number_zero_crossings'].data,label='N ()',color='m')\n",
    "axes[3].set_title('Number of waves crossing the zero ()')\n",
    "\n",
    "plt.xlim('2015-12-13T00:00:00','2015-12-29T00:00:00')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now the overlapping deployments are clear and the number of wave cross zero changes between deployment 1 and 2. There are no metadata available describing the time interval used to compute the wave statistics.  It is most likely that deployment 1 uses a longer averaging time window compared to deployment 2."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Comparison of data with external sources\n",
    "We made three different attempts to compare wind speeds and wave parameters to external data sources.\n",
    " \n",
    " * The first analysis uses the wind anemometer from the ship to compare to U10N during the  cruise on the Nathaniel Palmer during December 2015.\n",
    " * Next we compare the wind speeds and wave parameters from altimeters to the sensors of the Global Southern Ocean array.\n",
    " * Finally we compare the wind speeds and wave parameters from a wave hindcast to the sensors of the Global Southern Ocean array.\n",
    "\n",
    "Note it was difficult to perform the analysis in python.  See the supporting document for the Figures and a more detailed description. \n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Summary and Outlook\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Summary**\n",
    "\n",
    "Based on this analysis of the Global Southern Ocean surface wave buoy, we can summarize that the buoy measurements are of good quality. Here we summarize the findings:\n",
    "* The underestimation of the U10 was confirmed comparing the ship during December 2015. Our analysis shows that the underestimation could be has high as 20%, but this was based only on 50 data points.\n",
    "* The comparison between the altimeters shows: Hs, U10, and mean squared acceleration (mss) are all within a reasonable range of the buoy observations. However some larger discrepancies exist and Hs residuals of up 2 m are probable. At this stage it is not clear whether the buoy or altimeter or a combination of both contribute to these discrepancies. The residuals were largest under high sea states. The U10 from altimeter had a positive bias compared to the buoy consistent with the idea that the buoy might be underestimating U10.  The mss had the largest differences which could be reflective of buoy not being capable of capturing high frequency waves (due to its large size).\n",
    "* The comparison between the model and buoy shows that the all buoy sea state parameters are within an acceptable range. In other words there were no obvious outliers. The U10 CFSR bias using the buoy as reference was only 0.4 m/s which is consistent that the buoy is underestimating U10. \n",
    "* The time period the buoy recorded data and statistics were computed was not specified in any metadata. However it is likely the period was longer for deployment 1 compared to deployment 2. This was clearly demonstrated by jump in the number of waves crossing zero.\n",
    "* It is not clear how the damage to the buoy and antenna denoted in the annotation affect the quality of the observations.\n",
    "\n",
    "**Outlook**\n",
    "\n",
    "One of the purposes was to use this site to compare satellite observations. It seems the buoy data are of good quality and no strong biases were seen with respect to altimeters or models. So it is likely this data could be used to more rigorously develop techniques to improve satellite estimation at high sea states.\n",
    "\n",
    "When the sea state is particular large the buoy can be tilted and a bias (presumably underestimation) of the Hs can occur. This was first demonstrated when Hurricane Katrina traveled directly over a buoy from the NDBC network. This was described in the paper by Bender et al., JTECH 2010 and will be further explored for some of the extreme events. An additional motion package was mounted on the buoy and could be used a cross reference.\n",
    "\n",
    "This remote location is of high interest to have a continued time series available since there are often high sea states."
   ]
  }
 ]
}
